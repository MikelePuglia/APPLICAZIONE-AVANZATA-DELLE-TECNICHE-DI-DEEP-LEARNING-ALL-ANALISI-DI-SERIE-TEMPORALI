{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfa5b10",
   "metadata": {},
   "source": [
    "# 1. Imports and Setup\n",
    "\n",
    "In this cell, we import all the necessary libraries and set up the working environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-17T12:46:09.516060Z",
     "start_time": "2024-09-17T12:46:09.510799Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# TensorFlow and Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ReduceLROnPlateau, CSVLogger, ModelCheckpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebb0f76e4b2e6dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T12:34:07.984758Z",
     "start_time": "2024-09-16T12:34:07.961178Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to load configuration from a JSON file\n",
    "def load_config(filename: str = \"config.json\") -> dict:\n",
    "    \"\"\"\n",
    "    Loads the configuration from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Configuration parameters loaded from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7303cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuration and Parameters\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "INPUT_SHAPE = (32, 32, 1)\n",
    "EPOCHS = config[\"unet\"][\"decoder\"][\"training\"][\"epochs\"]\n",
    "\n",
    "### Paths\n",
    "# Define directories for data and results\n",
    "data_dir = \"/media/neurone-pc6/Volume/Michele/Prog_GAF_Michele/pythonProject/data/GASF\"\n",
    "label_0_folder = os.path.join(data_dir, \"Label_0\")\n",
    "label_1_folder = os.path.join(data_dir, \"Label_1\")\n",
    "results_dir = 'results'\n",
    "models_dir = 'models'\n",
    "logger_path = os.path.join(results_dir, 'classification_gasf.log')\n",
    "model_save_path = os.path.join(models_dir, 'classification_gasf.h5')\n",
    "\n",
    "#Definizione di ottimizzatore,loss e metriche\n",
    "OPTIMIZER = tf.keras.optimizers.get({\n",
    "    \"class_name\": config[\"unet\"][\"training\"][\"optimizer\"],\n",
    "    \"config\": {\n",
    "        \"learning_rate\": config[\"unet\"][\"training\"][\"learning_rate\"]\n",
    "    }\n",
    "})\n",
    "\n",
    "LOSS = config[\"unet\"][\"decoder\"][\"training\"][\"loss\"] ,\n",
    "METRICS = config[\"unet\"][\"decoder\"][\"metrics\"]\n",
    "\n",
    "#Definizione dei callbacks\n",
    "classification_model_checkpoint = ModelCheckpoint(\n",
    "    filepath='models/classification_gasf_checkpoint.h5',  # Specifica un nome di file diverso per il checkpoint\n",
    "    monitor=config[\"unet\"][\"training\"][\"model_checkpoint\"][\"monitor\"],\n",
    "    save_best_only=config[\"unet\"][\"training\"][\"model_checkpoint\"][\"save_best_only\"]\n",
    ")\n",
    "\n",
    "classification_early_stopping = EarlyStopping(\n",
    "    monitor=config[\"unet\"][\"training\"][\"early_stopping\"][\"monitor\"],\n",
    "    patience=config[\"unet\"][\"training\"][\"early_stopping\"][\"patience\"],\n",
    "    restore_best_weights=config[\"unet\"][\"training\"][\"early_stopping\"][\"restore_best_weights\"]\n",
    ")\n",
    "\n",
    "classification_reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=config[\"unet\"][\"training\"][\"lr_scheduler\"][\"monitor\"],\n",
    "    factor=config[\"unet\"][\"training\"][\"lr_scheduler\"][\"factor\"],\n",
    "    patience=config[\"unet\"][\"training\"][\"lr_scheduler\"][\"patience\"]\n",
    ")\n",
    "\n",
    "classification_csv_logger = CSVLogger('results/classification_gasf.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd837f",
   "metadata": {},
   "source": [
    "# 2. Utility Functions\n",
    "\n",
    "In this section, we define the utility functions for loading configuration, data, and creating the CNN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c917b8d1a0451fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T12:34:20.136813Z",
     "start_time": "2024-09-16T12:34:20.132701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create a DataFrame with image paths and labels\n",
    "def create_dataframe(label_0_folder, label_1_folder):\n",
    "    \"\"\"\n",
    "    Create a DataFrame mapping image paths to their labels.\n",
    "\n",
    "    Args:\n",
    "        label_0_folder (str): Path to folder containing class 0 images.\n",
    "        label_1_folder (str): Path to folder containing class 1 images.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns [\"image_path\", \"label\"].\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for filename in os.listdir(label_0_folder):\n",
    "        img_path = os.path.join(label_0_folder, filename)\n",
    "        data.append((img_path, 0))\n",
    "\n",
    "    for filename in os.listdir(label_1_folder):\n",
    "        img_path = os.path.join(label_1_folder, filename)\n",
    "        data.append((img_path, 1))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"image_path\", \"label\"])\n",
    "    return shuffle(df).reset_index(drop=True)\n",
    "\n",
    "# Function to preprocess and load classification images\n",
    "def load_and_preprocess_image_classification(image_path, label):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image for classification.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        label (int): Label corresponding to the image.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor, int: Processed image tensor and its label.\n",
    "    \"\"\"\n",
    "    def _load_image(image_path):\n",
    "        image = np.load(image_path.decode('utf-8')).astype(np.float32)\n",
    "        image = (image + 1) / 2.0  # Normalize from [-1, 1] to [0, 1]\n",
    "        if image.ndim == 2:\n",
    "            image = np.expand_dims(image, axis=-1)  # Add channel dimension if grayscale\n",
    "        return image\n",
    "\n",
    "    image = tf.numpy_function(_load_image, [image_path], tf.float32)\n",
    "    image.set_shape([32, 32, 1])  # Set explicit shape for TensorFlow\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43760cb1",
   "metadata": {},
   "source": [
    "# 3 Dataset creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9120e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create DataFrame\n",
    "# Map image paths to labels\n",
    "df = create_dataframe(label_0_folder, label_1_folder)\n",
    "df[\"label\"] = df[\"label\"].astype(np.float32)\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "### TensorFlow Datasets\n",
    "# Prepare TensorFlow datasets for training, validation, and testing\n",
    "train_ds_classification = tf.data.Dataset.from_tensor_slices((train_df[\"image_path\"].values, train_df[\"label\"].values))\n",
    "train_ds_classification = train_ds_classification.map(load_and_preprocess_image_classification, num_parallel_calls=AUTOTUNE)\n",
    "train_ds_classification = train_ds_classification.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds_classification = tf.data.Dataset.from_tensor_slices((val_df[\"image_path\"].values, val_df[\"label\"].values))\n",
    "val_ds_classification = val_ds_classification.map(load_and_preprocess_image_classification, num_parallel_calls=AUTOTUNE)\n",
    "val_ds_classification = val_ds_classification.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds_classification = tf.data.Dataset.from_tensor_slices((test_df[\"image_path\"].values, test_df[\"label\"].values))\n",
    "test_ds_classification = test_ds_classification.map(load_and_preprocess_image_classification, num_parallel_calls=AUTOTUNE)\n",
    "test_ds_classification = test_ds_classification.batch(1).prefetch(AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364cd47",
   "metadata": {},
   "source": [
    "# 5. Model Training and Evaluation\n",
    "\n",
    "We will now train the Cnn model and evaluate it on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8c3a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passi di train e validation\n",
    "STEPS_PER_EPOCH = tf.data.experimental.cardinality(train_ds_classification).numpy()\n",
    "VALIDATION_STEPS = tf.data.experimental.cardinality(val_ds_classification).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f8f49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained U-Net model\n",
    "unet_model = load_model('models/u-net_gasf.h5')\n",
    "\n",
    "def create_classification_model(base_model: Model, dense_units: int = 256, dropout_rate: float = 0.5) -> Model:\n",
    "    \"\"\"\n",
    "    Creates a classification model on top of a pre-trained base model (e.g., U-Net).\n",
    "\n",
    "    Parameters:\n",
    "    - base_model (Model): The pre-trained model to use as a base.\n",
    "    - dense_units (int): Number of units in the dense layer.\n",
    "    - dropout_rate (float): Dropout rate for regularization.\n",
    "\n",
    "    Returns:\n",
    "    - Model: The classification model.\n",
    "    \"\"\"\n",
    "    # Freeze encoder layers\n",
    "    for layer in base_model.layers[:-9]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add classification head\n",
    "    flatten_layer = layers.Flatten()(base_model.output)\n",
    "    dense_layer = layers.Dense(dense_units, activation='relu', \n",
    "                               kernel_regularizer=tf.keras.regularizers.l2(0.001))(flatten_layer)\n",
    "    dropout_layer = layers.Dropout(dropout_rate)(dense_layer)\n",
    "    output_layer = layers.Dense(1, activation='sigmoid')(dropout_layer)\n",
    "\n",
    "    return models.Model(inputs=base_model.input, outputs=output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3311eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: Model, train_ds: tf.data.Dataset, val_ds: tf.data.Dataset, \n",
    "                epochs: int, optimizer: tf.keras.optimizers.Optimizer, \n",
    "                loss: str, metrics: list[str], callbacks: list[tf.keras.callbacks.Callback]) -> tf.keras.callbacks.History:\n",
    "    \"\"\"\n",
    "    Compiles and trains a model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (Model): The model to train.\n",
    "    - train_ds (tf.data.Dataset): The training dataset.\n",
    "    - val_ds (tf.data.Dataset): The validation dataset.\n",
    "    - epochs (int): Number of epochs to train the model.\n",
    "    - optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for training.\n",
    "    - loss (str): The loss function to use for training.\n",
    "    - metrics (list[str]): A list of metrics to evaluate during training.\n",
    "    - callbacks (list[tf.keras.callbacks.Callback]): A list of callbacks to use during training.\n",
    "\n",
    "    Returns:\n",
    "    - tf.keras.callbacks.History: The history object containing the training details.\n",
    "    \"\"\"\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def evaluate_and_save_results(model: Model, test_ds: tf.data.Dataset, metrics: list[str], results_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and saves the results to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - model (Model): The trained model to evaluate.\n",
    "    - test_ds (tf.data.Dataset): The test dataset.\n",
    "    - metrics (list[str]): A list of metrics to include in the evaluation.\n",
    "    - results_dir (str): Directory to save the evaluation results.\n",
    "    \"\"\"\n",
    "    test_results = model.evaluate(test_ds)\n",
    "\n",
    "    print(\"Test Loss:\", test_results[0])\n",
    "    print(\"Test Accuracy:\", test_results[1])\n",
    "\n",
    "    test_results_path = os.path.join(results_dir, 'test_results_classification_gasf.txt')\n",
    "    with open(test_results_path, 'w') as f:\n",
    "        f.write(f\"Test Loss: {test_results[0]}\\n\")\n",
    "        for i, metric in enumerate(metrics):\n",
    "            f.write(f\"Test {metric}: {test_results[i + 1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaba988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification model\n",
    "classification_model = create_classification_model(unet_model, dense_units=256, dropout_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "61420a54f6c54490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T12:34:37.986614Z",
     "start_time": "2024-09-16T12:34:37.950283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 32)   320         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 32)   9248        ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 16, 16, 32)   0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 16, 16, 64)   18496       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 16, 64)   36928       ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 64)    0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 8, 8, 128)    73856       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 8, 8, 128)    147584      ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " up_sampling2d (UpSampling2D)   (None, 16, 16, 128)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " tf.image.resize (TFOpLambda)   (None, 16, 16, 128)  0           ['up_sampling2d[0][0]']          \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 16, 16, 192)  0           ['tf.image.resize[0][0]',        \n",
      "                                                                  'conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 16, 16, 64)   110656      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 64)   36928       ['conv2d_6[0][0]']               \n",
      "                                                                                                  \n",
      " up_sampling2d_1 (UpSampling2D)  (None, 32, 32, 64)  0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " tf.image.resize_1 (TFOpLambda)  (None, 32, 32, 64)  0           ['up_sampling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 32, 32, 96)   0           ['tf.image.resize_1[0][0]',      \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)              (None, 32, 32, 32)   27680       ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)              (None, 32, 32, 32)   9248        ['conv2d_8[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)             (None, 32, 32, 1)    33          ['conv2d_9[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1024)         0           ['conv2d_10[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          262400      ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 256)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            257         ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 733,634\n",
      "Trainable params: 447,202\n",
      "Non-trainable params: 286,432\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classification_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc13e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classification model\n",
    "classification_history = train_model(\n",
    "    model=classification_model,\n",
    "    train_ds=train_ds_classification,\n",
    "    val_ds=val_ds_classification,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=OPTIMIZER,\n",
    "    loss=LOSS,\n",
    "    metrics=METRICS,\n",
    "    callbacks=[classification_early_stopping, classification_model_checkpoint, classification_reduce_lr, classification_csv_logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c319280",
   "metadata": {},
   "source": [
    "3500/3500 [==============================] - 2104s 600ms/step - loss: 0.7670 - accuracy: 0.5016 - val_loss: 0.6937 - val_accuracy: 0.5015 - lr: 1.0000e-04\n",
    "Epoch 2/30\n",
    "3500/3500 [==============================] - 2235s 639ms/step - loss: 0.6932 - accuracy: 0.4995 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 1.0000e-04\n",
    "Epoch 3/30\n",
    "3500/3500 [==============================] - 1421s 406ms/step - loss: 0.6932 - accuracy: 0.5004 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 1.0000e-04\n",
    "Epoch 4/30\n",
    "3500/3500 [==============================] - 478s 137ms/step - loss: 0.6932 - accuracy: 0.4987 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 1.0000e-04\n",
    "Epoch 5/30\n",
    "3500/3500 [==============================] - 79s 23ms/step - loss: 0.6931 - accuracy: 0.5013 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 1.0000e-04\n",
    "Epoch 6/30\n",
    "3500/3500 [==============================] - 74s 21ms/step - loss: 0.6932 - accuracy: 0.4980 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 1.0000e-04\n",
    "Epoch 7/30\n",
    "3500/3500 [==============================] - 78s 22ms/step - loss: 0.6932 - accuracy: 0.4996 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 1.0000e-04\n",
    "Epoch 8/30\n",
    "3500/3500 [==============================] - 71s 20ms/step - loss: 0.6932 - accuracy: 0.4977 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 5.0000e-05\n",
    "Epoch 9/30\n",
    "3500/3500 [==============================] - 69s 20ms/step - loss: 0.6932 - accuracy: 0.4992 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 5.0000e-05\n",
    "Epoch 10/30\n",
    "3500/3500 [==============================] - 71s 20ms/step - loss: 0.6931 - accuracy: 0.5011 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 5.0000e-05\n",
    "Epoch 11/30\n",
    "3500/3500 [==============================] - 69s 20ms/step - loss: 0.6932 - accuracy: 0.4988 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 5.0000e-05\n",
    "Epoch 12/30\n",
    "3500/3500 [==============================] - 69s 20ms/step - loss: 0.6932 - accuracy: 0.4989 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 5.0000e-05\n",
    "Epoch 13/30\n",
    "3500/3500 [==============================] - 68s 20ms/step - loss: 0.6932 - accuracy: 0.4985 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 2.5000e-05\n",
    "Epoch 14/30\n",
    "3500/3500 [==============================] - 69s 20ms/step - loss: 0.6931 - accuracy: 0.4987 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 2.5000e-05\n",
    "Epoch 15/30\n",
    "3500/3500 [==============================] - 71s 20ms/step - loss: 0.6932 - accuracy: 0.4991 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 2.5000e-05\n",
    "Epoch 16/30\n",
    "3500/3500 [==============================] - 69s 20ms/step - loss: 0.6931 - accuracy: 0.5000 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 2.5000e-05\n",
    "Epoch 17/30\n",
    "3500/3500 [==============================] - 68s 20ms/step - loss: 0.6931 - accuracy: 0.5023 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 2.5000e-05\n",
    "Epoch 18/30\n",
    "3500/3500 [==============================] - 68s 20ms/step - loss: 0.6932 - accuracy: 0.5003 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 1.2500e-05\n",
    "Epoch 19/30\n",
    "3500/3500 [==============================] - 73s 21ms/step - loss: 0.6932 - accuracy: 0.5008 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 1.2500e-05\n",
    "Epoch 20/30\n",
    "3500/3500 [==============================] - 68s 20ms/step - loss: 0.6931 - accuracy: 0.5011 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 1.2500e-05\n",
    "Epoch 21/30\n",
    "3500/3500 [==============================] - 68s 20ms/step - loss: 0.6932 - accuracy: 0.4986 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 1.2500e-05\n",
    "Epoch 22/30\n",
    "3500/3500 [==============================] - 68s 19ms/step - loss: 0.6932 - accuracy: 0.5001 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 1.2500e-05\n",
    "Epoch 23/30\n",
    "3500/3500 [==============================] - 68s 20ms/step - loss: 0.6931 - accuracy: 0.5002 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 6.2500e-06\n",
    "Epoch 24/30\n",
    "3500/3500 [==============================] - 68s 19ms/step - loss: 0.6932 - accuracy: 0.5007 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 6.2500e-06\n",
    "Epoch 25/30\n",
    "3500/3500 [==============================] - 68s 19ms/step - loss: 0.6931 - accuracy: 0.4977 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 6.2500e-06\n",
    "Epoch 26/30\n",
    "3500/3500 [==============================] - 68s 19ms/step - loss: 0.6931 - accuracy: 0.5011 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 6.2500e-06\n",
    "Epoch 27/30\n",
    "3500/3500 [==============================] - 68s 19ms/step - loss: 0.6931 - accuracy: 0.5023 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 6.2500e-06\n",
    "Epoch 28/30\n",
    "3500/3500 [==============================] - 71s 20ms/step - loss: 0.6932 - accuracy: 0.4998 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 3.1250e-06\n",
    "Epoch 29/30\n",
    "3500/3500 [==============================] - 68s 19ms/step - loss: 0.6931 - accuracy: 0.5020 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 3.1250e-06\n",
    "Epoch 30/30\n",
    "3500/3500 [==============================] - 68s 19ms/step - loss: 0.6931 - accuracy: 0.4999 - val_loss: 0.6931 - val_accuracy: 0.5015 - lr: 3.1250e-06\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c734303e58618",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T17:59:42.308069Z",
     "start_time": "2024-09-16T17:53:47.581294Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluate and save the results\n",
    "evaluate_and_save_results(\n",
    "    model=classification_model,\n",
    "    test_ds=test_ds_classification,\n",
    "    metrics=METRICS,\n",
    "    results_dir=results_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24000/24000 [==============================] - 497s 21ms/step - loss: 0.6933 - accuracy: 0.5016\n",
    "Test Loss: 0.693253755569458\n",
    "Test Accuracy: 0.5016250014305115"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437e525d",
   "metadata": {},
   "source": [
    "# 6. Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f34f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate and plot the confusion matrix\n",
    "def plot_confusion_matrix(test_ds, model, class_labels):\n",
    "    \"\"\"\n",
    "    Calculate and plot the confusion matrix with percentages for a classification model.\n",
    "\n",
    "    Parameters:\n",
    "    - test_ds (Dataset): A dataset containing test image batches and their corresponding labels.\n",
    "    - model (Model): The trained classification model.\n",
    "    - class_labels (list): A list of strings representing class labels for the confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "    - cm (ndarray): The confusion matrix as a 2D numpy array.\n",
    "    - cm_percent (ndarray): The confusion matrix with percentages as a 2D numpy array.\n",
    "    \"\"\"\n",
    "    # Collect ground truth labels (y_true) and model predictions (y_pred)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for image_batch, label_batch in test_ds:\n",
    "        predictions = model.predict(image_batch)\n",
    "        y_true.extend(label_batch.numpy())  # Append ground truth labels\n",
    "        y_pred.extend([round(pred.item()) for pred in predictions])  # Append rounded predictions\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calculate percentage for each cell relative to the total samples for that class\n",
    "    cm_percent = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "    # Plot the confusion matrix with both absolute numbers and percentages\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='', cmap='Blues',\n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "    # Add percentages to the heatmap\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            plt.text(j + 0.25, i + 0.5, f'{cm_percent[i][j]:.2f}%', \n",
    "                     ha='center', va='center', color='black')\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix (with Percentages)')\n",
    "    plt.show()\n",
    "\n",
    "    return cm, cm_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e41948f2440e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-16T18:49:49.826845Z",
     "start_time": "2024-09-16T18:49:48.908906Z"
    }
   },
   "outputs": [],
   "source": [
    "class_labels = ['Class 0', 'Class 1']\n",
    "cm, cm_percent = plot_confusion_matrix(test_ds_classification, classification_model, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbe1641",
   "metadata": {},
   "source": [
    "# 6. Model Saving\n",
    "\n",
    "After training, we save the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: Model, model_save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    - model (Model): The trained model to save.\n",
    "    - model_save_path (str): Path where the model will be saved.\n",
    "    \"\"\"\n",
    "    # Save the model to the specified path\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the trained model\n",
    "save_model(classification_model, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
