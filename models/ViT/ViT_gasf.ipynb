{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501ffd57",
   "metadata": {},
   "source": [
    "# 1. Imports and Setup\n",
    "\n",
    "In this cell, we import all the necessary libraries and set up the working environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537175dbd8ae78b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T17:36:10.758046Z",
     "start_time": "2024-09-01T17:36:07.551837Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007c13dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load configuration from a JSON file\n",
    "def load_config(filename: str = \"config.json\") -> dict:\n",
    "    \"\"\"\n",
    "    Loads the configuration from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Configuration parameters loaded from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2054dd1161ac7755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T17:36:11.022958Z",
     "start_time": "2024-09-01T17:36:11.019795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define global parameters\n",
    "batch_size = config[\"transformers\"][\"batch_size\"]  # Batch size for training\n",
    "num_epochs = config[\"transformers\"][\"training\"][\"epochs\"]  # Number of training epochs\n",
    "image_size = config[\"transformers\"][\"image_size\"]  # Image dimensions\n",
    "metrics = config[\"transformers\"][\"training\"][\"metrics\"]\n",
    "loss = config[\"transformers\"][\"training\"][\"loss\"]\n",
    "optimizer = config[\"transformers\"][\"training\"][\"optimizer\"] \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# ViT-specific parameters\n",
    "PATCH_SIZE = config[\"transformers\"][\"training\"][\"patch_size\"]\n",
    "PROJECTION_DIM = config[\"transformers\"][\"training\"][\"projection_dim\"]\n",
    "NUM_HEADS = config[\"transformers\"][\"training\"][\"num_heads\"]\n",
    "TRANSFORMER_UNITS = config[\"transformers\"][\"training\"][\"trasnformer_unit\"]\n",
    "TRANSFORMER_LAYERS = config[\"transformers\"][\"training\"][\"transformer_layer\"] \n",
    "MLP_HEAD_UNITS = config[\"transformers\"][\"training\"][\"mlp_head_units\"]\n",
    "NUM_CLASSES = config[\"transformers\"][\"training\"][\"num_classes\"] # Binary classification\n",
    "NUM_PATCHES = config[\"transformers\"][\"training\"][\"num_patches\"] \n",
    "\n",
    "model_save_path = \"models/vit_gasf.h5\"\n",
    "\n",
    "# Define callbacks\n",
    "classification_model_checkpoint = config[\"transformers\"][\"training\"][\"model_checkpoint\"] \n",
    "\n",
    "early_stopping = config[\"transformers\"][\"training\"][\"early_stopping\"]\n",
    "\n",
    "reduce_lr_on_plateau = config[\"transformers\"][\"training\"][\"lr_scheduler\"]\n",
    "\n",
    "csv_logger = config[\"transformers\"][\"training\"][\"lr_scheduler\"]\n",
    "\n",
    "callbacks = [\n",
    "    classification_model_checkpoint,\n",
    "    early_stopping,\n",
    "    reduce_lr_on_plateau,\n",
    "    csv_logger\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466009a7",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86814bc32dff71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T17:36:11.069997Z",
     "start_time": "2024-09-01T17:36:11.064197Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataframe mapping image paths to their labels\n",
    "def create_dataframe(label_0_folder, label_1_folder):\n",
    "    \"\"\"\n",
    "    Create a Pandas dataframe containing image paths and their corresponding labels.\n",
    "\n",
    "    Parameters:\n",
    "        label_0_folder (str): Path to the folder containing class 0 images.\n",
    "        label_1_folder (str): Path to the folder containing class 1 images.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with columns `image_path` and `label`.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for filename in os.listdir(label_0_folder):\n",
    "        img_path = os.path.join(label_0_folder, filename)\n",
    "        data.append((img_path, 0))\n",
    "\n",
    "    for filename in os.listdir(label_1_folder):\n",
    "        img_path = os.path.join(label_1_folder, filename)\n",
    "        data.append((img_path, 1))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"image_path\", \"label\"])\n",
    "    return df\n",
    "\n",
    "# %%\n",
    "# Load and preprocess images for classification\n",
    "def load_and_preprocess_image_classification(image_path, label):\n",
    "    \"\"\"\n",
    "    Load and preprocess images for classification tasks.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Path to the image file (numpy format).\n",
    "        label (float): Corresponding label for the image.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[tf.Tensor, float]: Preprocessed image tensor and its label.\n",
    "    \"\"\"\n",
    "    def _load_image(image_path):\n",
    "        image = np.load(image_path.decode('utf-8'))\n",
    "        image = image.astype(np.float32)\n",
    "        image = (image + 1) / 2.0  # Normalize from [-1, 1] to [0, 1]\n",
    "        if image.ndim == 2:  # Grayscale images\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "        return image\n",
    "\n",
    "    image = tf.numpy_function(_load_image, [image_path], tf.float32)\n",
    "    image.set_shape([32, 32, 1])\n",
    "    image = tf.image.resize(image, [image_size, image_size])\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802270ec2930990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T17:36:56.464251Z",
     "start_time": "2024-09-01T17:36:55.367200Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 19:36:55.850053: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-01 19:36:56.035447: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-01 19:36:56.035619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-01 19:36:56.121808: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-01 19:36:56.122072: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-01 19:36:56.122262: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-01 19:36:56.224687: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-01 19:36:56.224879: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-01 19:36:56.225014: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-09-01 19:36:56.225114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 167 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "# Paths to data folders\n",
    "data_dir = \"/media/neurone-pc6/Volume/Michele/Prog_GAF_Michele/pythonProject/data/GASF\"\n",
    "label_0_folder = os.path.join(data_dir, \"Label_0\")\n",
    "label_1_folder = os.path.join(data_dir, \"Label_1\")\n",
    "\n",
    "# Create dataframe\n",
    "df = create_dataframe(label_0_folder, label_1_folder)\n",
    "df[\"label\"] = df[\"label\"].astype(np.float32)\n",
    "\n",
    "# Split dataset\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_ds_classification = tf.data.Dataset.from_tensor_slices((train_df[\"image_path\"].values, train_df[\"label\"].values))\n",
    "train_ds_classification = train_ds_classification.map(load_and_preprocess_image_classification, num_parallel_calls=AUTOTUNE).batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds_classification = tf.data.Dataset.from_tensor_slices((val_df[\"image_path\"].values, val_df[\"label\"].values))\n",
    "val_ds_classification = val_ds_classification.map(load_and_preprocess_image_classification, num_parallel_calls=AUTOTUNE).batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds_classification = tf.data.Dataset.from_tensor_slices((test_df[\"image_path\"].values, test_df[\"label\"].values))\n",
    "test_ds_classification = test_ds_classification.map(load_and_preprocess_image_classification, num_parallel_calls=AUTOTUNE).batch(1).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838640ca",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8ed332d0ea616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T17:37:01.492124Z",
     "start_time": "2024-09-01T17:37:01.481238Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Patch Encoder class\n",
    "class PatchEncoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer for patch encoding in Vision Transformers.\n",
    "\n",
    "    Parameters:\n",
    "        num_patches (int): Total number of patches in the image.\n",
    "        projection_dim (int): Dimensionality of the projection space.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "# %%\n",
    "# Create the Vision Transformer (ViT) model\n",
    "def create_vit_classifier():\n",
    "    \"\"\"\n",
    "    Build a Vision Transformer (ViT) model for binary classification.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled ViT model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(image_size, image_size, 1))\n",
    "\n",
    "    # Patch extraction\n",
    "    x = layers.Conv2D(filters=PROJECTION_DIM, kernel_size=PATCH_SIZE, strides=PATCH_SIZE, padding='valid')(inputs)\n",
    "    x = layers.Reshape((NUM_PATCHES, PROJECTION_DIM))(x)\n",
    "\n",
    "    # Positional Encoding\n",
    "    x = PatchEncoder(NUM_PATCHES, PROJECTION_DIM)(x)\n",
    "\n",
    "    # Transformer Encoder\n",
    "    for _ in range(TRANSFORMER_LAYERS):\n",
    "        x_res = x\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = layers.MultiHeadAttention(key_dim=PROJECTION_DIM, num_heads=NUM_HEADS, dropout=0.1)(x, x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.Add()([x, x_res])\n",
    "\n",
    "        x_res = x\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = layers.Dense(TRANSFORMER_UNITS[0], activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.Dense(TRANSFORMER_UNITS[1])(x)\n",
    "        x = layers.Add()([x, x_res])\n",
    "\n",
    "    # MLP Head\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for units in MLP_HEAD_UNITS:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae0b23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64386b53589898",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T17:37:03.528955Z",
     "start_time": "2024-09-01T17:37:02.742969Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 1)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 8, 8, 64)     1088        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 64, 64)       0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 64, 64)       8256        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 64, 64)      128         ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 64, 64)      66368       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64, 64)       0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 64, 64)       0           ['dropout[0][0]',                \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 64, 64)      128         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64, 128)      8320        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64, 128)      0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64, 64)       8256        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 64, 64)       0           ['dense_2[0][0]',                \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 64, 64)      128         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 64, 64)      66368       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64, 64)       0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 64, 64)       0           ['dropout_2[0][0]',              \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 64, 64)      128         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64, 128)      8320        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64, 128)      0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64, 64)       8256        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 64, 64)       0           ['dense_4[0][0]',                \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 64, 64)      128         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 64, 64)      66368       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64, 64)       0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 64, 64)       0           ['dropout_4[0][0]',              \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 64, 64)      128         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64, 128)      8320        ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 64, 128)      0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64, 64)       8256        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 64, 64)       0           ['dense_6[0][0]',                \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 64, 64)      128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 64, 64)      66368       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 64, 64)       0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 64, 64)       0           ['dropout_6[0][0]',              \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 64, 64)      128         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64, 128)      8320        ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64, 128)      0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 64, 64)       8256        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 64, 64)       0           ['dense_8[0][0]',                \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 64)          0           ['add_7[0][0]']                  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 128)          8320        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64)           8256        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 64)           0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1)            65          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 358,785\n",
      "Trainable params: 358,785\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(\"Model Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d4ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: tf.keras.Model, train_ds: tf.data.Dataset, val_ds: tf.data.Dataset, \n",
    "                epochs: int, optimizer: tf.keras.optimizers.Optimizer, \n",
    "                loss: str, metrics: list[str], callbacks: list[tf.keras.callbacks.Callback]) -> tf.keras.callbacks.History:\n",
    "    \"\"\"\n",
    "    Compiles and trains a model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (tf.keras.Model): The model to train.\n",
    "    - train_ds (tf.data.Dataset): The training dataset.\n",
    "    - val_ds (tf.data.Dataset): The validation dataset.\n",
    "    - epochs (int): Number of epochs to train the model.\n",
    "    - optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for training.\n",
    "    - loss (str): The loss function to use for training.\n",
    "    - metrics (list[str]): A list of metrics to evaluate during training.\n",
    "    - callbacks (list[tf.keras.callbacks.Callback]): A list of callbacks to use during training.\n",
    "\n",
    "    Returns:\n",
    "    - tf.keras.callbacks.History: The history object containing the training details.\n",
    "    \"\"\"\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    return history\n",
    "\n",
    "def evaluate_and_save_results(model: tf.keras.Model, test_ds: tf.data.Dataset, metrics: list[str], results_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and saves the results to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - model (tf.keras.Model): The trained model to evaluate.\n",
    "    - test_ds (tf.data.Dataset): The test dataset.\n",
    "    - metrics (list[str]): A list of metrics to include in the evaluation.\n",
    "    - results_dir (str): Directory to save the evaluation results.\n",
    "    \"\"\"\n",
    "    test_results = model.evaluate(test_ds)\n",
    "\n",
    "    print(\"Test Loss:\", test_results[0])\n",
    "    for i, metric in enumerate(metrics):\n",
    "        print(f\"Test {metric}:\", test_results[i + 1])\n",
    "\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    results_path = os.path.join(results_dir, 'test_results_vit_gasf.txt')\n",
    "    with open(results_path, 'w') as f:\n",
    "        f.write(f\"Test Loss: {test_results[0]}\\n\")\n",
    "        for i, metric in enumerate(metrics):\n",
    "            f.write(f\"Test {metric}: {test_results[i + 1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bd8871442e74a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T19:29:12.768285Z",
     "start_time": "2024-09-01T17:37:05.451310Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 19:37:05.489344: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [112000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-09-01 19:37:05.489556: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [112000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-09-01 19:37:11.710149: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2024-09-01 19:37:12.334691: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-01 19:37:13.361654: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7ff31c299fb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-09-01 19:37:13.361736: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1\n",
      "2024-09-01 19:37:13.454671: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-01 19:37:14.032239: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-01 19:37:14.282133: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500/3500 [==============================] - ETA: 0s - loss: 0.7000 - accuracy: 0.4977"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 20:11:14.645349: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [24000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-09-01 20:11:14.646724: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [24000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500/3500 [==============================] - 2520s 716ms/step - loss: 0.7000 - accuracy: 0.4977 - val_loss: 0.6932 - val_accuracy: 0.5042\n",
      "Epoch 2/30\n",
      "3500/3500 [==============================] - 1595s 456ms/step - loss: 0.6958 - accuracy: 0.4999 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 3/30\n",
      "3500/3500 [==============================] - 455s 130ms/step - loss: 0.6944 - accuracy: 0.4983 - val_loss: 0.6932 - val_accuracy: 0.5042\n",
      "Epoch 4/30\n",
      "3500/3500 [==============================] - 209s 60ms/step - loss: 0.6936 - accuracy: 0.5031 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 5/30\n",
      "3500/3500 [==============================] - 77s 22ms/step - loss: 0.6933 - accuracy: 0.5023 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 6/30\n",
      "3500/3500 [==============================] - 74s 21ms/step - loss: 0.6932 - accuracy: 0.5039 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 7/30\n",
      "3500/3500 [==============================] - 73s 21ms/step - loss: 0.6932 - accuracy: 0.5045 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 8/30\n",
      "3500/3500 [==============================] - 74s 21ms/step - loss: 0.6932 - accuracy: 0.5036 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 9/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6931 - accuracy: 0.5036 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 10/30\n",
      "3500/3500 [==============================] - 76s 22ms/step - loss: 0.6932 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 11/30\n",
      "3500/3500 [==============================] - 76s 22ms/step - loss: 0.6932 - accuracy: 0.5025 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 12/30\n",
      "3500/3500 [==============================] - 76s 22ms/step - loss: 0.6932 - accuracy: 0.5031 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 13/30\n",
      "3500/3500 [==============================] - 76s 22ms/step - loss: 0.6932 - accuracy: 0.5016 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 14/30\n",
      "3500/3500 [==============================] - 75s 22ms/step - loss: 0.6931 - accuracy: 0.5037 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 15/30\n",
      "3500/3500 [==============================] - 75s 22ms/step - loss: 0.6932 - accuracy: 0.5045 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 16/30\n",
      "3500/3500 [==============================] - 75s 22ms/step - loss: 0.6931 - accuracy: 0.5041 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 17/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6931 - accuracy: 0.5037 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 18/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 19/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 20/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6931 - accuracy: 0.5036 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 21/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 22/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6931 - accuracy: 0.5036 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 23/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6931 - accuracy: 0.5040 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 24/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6931 - accuracy: 0.5037 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 25/30\n",
      "3500/3500 [==============================] - 74s 21ms/step - loss: 0.6931 - accuracy: 0.5034 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 26/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6931 - accuracy: 0.5033 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 27/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6931 - accuracy: 0.5035 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 28/30\n",
      "3500/3500 [==============================] - 74s 21ms/step - loss: 0.6933 - accuracy: 0.5029 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 29/30\n",
      "3500/3500 [==============================] - 75s 21ms/step - loss: 0.6932 - accuracy: 0.5032 - val_loss: 0.6931 - val_accuracy: 0.5042\n",
      "Epoch 30/30\n",
      "3500/3500 [==============================] - 74s 21ms/step - loss: 0.6931 - accuracy: 0.5038 - val_loss: 0.6931 - val_accuracy: 0.5042\n"
     ]
    }
   ],
   "source": [
    "# Addestramento del modello\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_ds=train_ds_classification,\n",
    "    val_ds=val_ds_classification,\n",
    "    epochs=num_epochs,\n",
    "    optimizer= optimizer,\n",
    "    loss= loss,\n",
    "    metrics= metrics,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f17622a2ded8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-01T19:36:18.500084Z",
     "start_time": "2024-09-01T19:29:13.126317Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-01 21:29:13.156830: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [24000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-09-01 21:29:13.157127: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [24000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/24000 [==============================] - 425s 18ms/step - loss: 0.6931 - accuracy: 0.5079\n",
      "Test Loss: 0.6930994391441345\n",
      "Test Accuracy: 0.5079166889190674\n"
     ]
    }
   ],
   "source": [
    "# Valutazione e visualizzazione dei risultati\n",
    "evaluate_and_save_results(\n",
    "    model=model,\n",
    "    test_ds=test_ds_classification,\n",
    "    metrics= metrics,\n",
    "    results_dir='results_vit'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4e112",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651e43154a3eda3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate and plot the confusion matrix\n",
    "def plot_confusion_matrix(test_ds, model, class_labels):\n",
    "    \"\"\"\n",
    "    Calculate and plot the confusion matrix with percentages for a classification model.\n",
    "\n",
    "    Parameters:\n",
    "    - test_ds (Dataset): A dataset containing test image batches and their corresponding labels.\n",
    "    - model (Model): The trained classification model.\n",
    "    - class_labels (list): A list of strings representing class labels for the confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "    - cm (ndarray): The confusion matrix as a 2D numpy array.\n",
    "    - cm_percent (ndarray): The confusion matrix with percentages as a 2D numpy array.\n",
    "    \"\"\"\n",
    "    # Collect ground truth labels (y_true) and model predictions (y_pred)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for image_batch, label_batch in test_ds:\n",
    "        predictions = model.predict(image_batch)\n",
    "        y_true.extend(label_batch.numpy())  # Append ground truth labels\n",
    "        y_pred.extend([round(pred.item()) for pred in predictions])  # Append rounded predictions\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calculate percentage for each cell relative to the total samples for that class\n",
    "    cm_percent = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "    # Plot the confusion matrix with both absolute numbers and percentages\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='', cmap='Blues',\n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "    # Add percentages to the heatmap\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            plt.text(j + 0.25, i + 0.5, f'{cm_percent[i][j]:.2f}%', \n",
    "                     ha='center', va='center', color='black')\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix (with Percentages)')\n",
    "    plt.show()\n",
    "\n",
    "    return cm, cm_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267756dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"Class 0\", \"Class 1\"]\n",
    "cm, cm_percent = plot_confusion_matrix(test_ds=test_ds_classification, model=model, class_labels=class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b25307",
   "metadata": {},
   "source": [
    "# 6. Model Saving\n",
    "\n",
    "After training, we save the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: tf.keras.Model, model_save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    - model (tf.keras.Model): The trained model to save.\n",
    "    - model_save_path (str): Path where the model will be saved.\n",
    "    \"\"\"\n",
    "    # Creazione della directory se non esiste\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "    # Save\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "save_model(model, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
