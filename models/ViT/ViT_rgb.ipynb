{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76f4e1d9",
   "metadata": {},
   "source": [
    "# 1. Imports and Setup\n",
    "\n",
    "In this cell, we import all the necessary libraries and set up the working environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3537175dbd8ae78b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:00:03.821169Z",
     "start_time": "2024-09-19T12:59:59.933530Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00466b137258395",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:00:05.666148Z",
     "start_time": "2024-09-19T13:00:05.645354Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to load configuration from a JSON file\n",
    "def load_config(filename: str = \"config.json\") -> dict:\n",
    "    \"\"\"\n",
    "    Loads the configuration from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Configuration parameters loaded from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2054dd1161ac7755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:00:05.679496Z",
     "start_time": "2024-09-19T13:00:05.676365Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define global parameters\n",
    "batch_size = config[\"transformers\"][\"batch_size\"]  # Batch size for training\n",
    "num_epochs = config[\"transformers\"][\"training\"][\"epochs\"]  # Number of training epochs\n",
    "image_size = config[\"transformers\"][\"image_size\"]  # Image dimensions\n",
    "metrics = config[\"transformers\"][\"training\"][\"metrics\"]\n",
    "loss = config[\"transformers\"][\"training\"][\"loss\"]\n",
    "optimizer = config[\"transformers\"][\"training\"][\"optimizer\"] \n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# ViT-specific parameters\n",
    "PATCH_SIZE = config[\"transformers\"][\"training\"][\"patch_size\"]\n",
    "PROJECTION_DIM = config[\"transformers\"][\"training\"][\"projection_dim\"]\n",
    "NUM_HEADS = config[\"transformers\"][\"training\"][\"num_heads\"]\n",
    "TRANSFORMER_UNITS = config[\"transformers\"][\"training\"][\"trasnformer_unit\"]\n",
    "TRANSFORMER_LAYERS = config[\"transformers\"][\"training\"][\"transformer_layer\"] \n",
    "MLP_HEAD_UNITS = config[\"transformers\"][\"training\"][\"mlp_head_units\"]\n",
    "NUM_CLASSES = config[\"transformers\"][\"training\"][\"num_classes\"] # Binary classification\n",
    "NUM_PATCHES = config[\"transformers\"][\"training\"][\"num_patches\"] \n",
    "\n",
    "model_save_path = \"models/vit_rgb.h5\"\n",
    "\n",
    "# Define callbacks\n",
    "classification_model_checkpoint = config[\"transformers\"][\"training\"][\"model_checkpoint\"] \n",
    "\n",
    "early_stopping = config[\"transformers\"][\"training\"][\"early_stopping\"]\n",
    "\n",
    "reduce_lr_on_plateau = config[\"transformers\"][\"training\"][\"lr_scheduler\"]\n",
    "\n",
    "csv_logger = config[\"transformers\"][\"training\"][\"lr_scheduler\"]\n",
    "\n",
    "callbacks = [\n",
    "    classification_model_checkpoint,\n",
    "    early_stopping,\n",
    "    reduce_lr_on_plateau,\n",
    "    csv_logger\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5000d69f",
   "metadata": {},
   "source": [
    "# 4 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b86814bc32dff71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:00:05.730138Z",
     "start_time": "2024-09-19T13:00:05.725449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataframe mapping image paths to their labels\n",
    "def create_dataframe(label_0_folder, label_1_folder):\n",
    "    \"\"\"\n",
    "    Create a Pandas dataframe containing image paths and their corresponding labels.\n",
    "\n",
    "    Parameters:\n",
    "        label_0_folder (str): Path to the folder containing class 0 images.\n",
    "        label_1_folder (str): Path to the folder containing class 1 images.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with columns `image_path` and `label`.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for filename in os.listdir(label_0_folder):\n",
    "        img_path = os.path.join(label_0_folder, filename)\n",
    "        data.append((img_path, 0))\n",
    "\n",
    "    for filename in os.listdir(label_1_folder):\n",
    "        img_path = os.path.join(label_1_folder, filename)\n",
    "        data.append((img_path, 1))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"image_path\", \"label\"])\n",
    "    return df\n",
    "\n",
    "# %%\n",
    "# Load and preprocess images for classification\n",
    "def load_and_preprocess_image_classification(image_path, label):\n",
    "    \"\"\"\n",
    "    Load and preprocess images for classification tasks.\n",
    "\n",
    "    Parameters:\n",
    "        image_path (str): Path to the image file (numpy format).\n",
    "        label (float): Corresponding label for the image.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[tf.Tensor, float]: Preprocessed image tensor and its label.\n",
    "    \"\"\"\n",
    "    def _load_image(image_path):\n",
    "        image = np.load(image_path.decode('utf-8'))\n",
    "        image = image.astype(np.float32)\n",
    "        image = (image + 1) / 2.0  # Normalize from [-1, 1] to [0, 1]\n",
    "        if image.ndim == 2:  # Grayscale images\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "        return image\n",
    "\n",
    "    image = tf.numpy_function(_load_image, [image_path], tf.float32)\n",
    "    image.set_shape([32, 32, 1])\n",
    "    image = tf.image.resize(image, [image_size, image_size])\n",
    "    return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802270ec2930990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:00:06.453298Z",
     "start_time": "2024-09-19T13:00:05.778910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Paths to data folders\n",
    "data_dir = \"/media/neurone-pc6/Volume/Michele/Prog_GAF_Michele/pythonProject/data/RGB\"\n",
    "label_0_folder = os.path.join(data_dir, \"Label_0\")\n",
    "label_1_folder = os.path.join(data_dir, \"Label_1\")\n",
    "\n",
    "# Create dataframe\n",
    "df = create_dataframe(label_0_folder, label_1_folder)\n",
    "df[\"label\"] = df[\"label\"].astype(np.float32)\n",
    "\n",
    "# Split dataset\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_ds_classification = tf.data.Dataset.from_tensor_slices((train_df[\"image_path\"].values, train_df[\"label\"].values))\n",
    "train_ds_classification = train_ds_classification.map(load_and_preprocess_image_classification, num_parallel_calls=AUTOTUNE).batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "val_ds_classification = tf.data.Dataset.from_tensor_slices((val_df[\"image_path\"].values, val_df[\"label\"].values))\n",
    "val_ds_classification = val_ds_classification.map(load_and_preprocess_image_classification, num_parallel_calls=AUTOTUNE).batch(batch_size).prefetch(AUTOTUNE)\n",
    "\n",
    "test_ds_classification = tf.data.Dataset.from_tensor_slices((test_df[\"image_path\"].values, test_df[\"label\"].values))\n",
    "test_ds_classification = test_ds_classification.map(load_and_preprocess_image_classification, num_parallel_calls=AUTOTUNE).batch(1).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4150811a",
   "metadata": {},
   "source": [
    "# 5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e8ed332d0ea616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:00:06.476597Z",
     "start_time": "2024-09-19T13:00:06.470401Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Patch Encoder class\n",
    "class PatchEncoder(layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom Keras layer for patch encoding in Vision Transformers.\n",
    "\n",
    "    Parameters:\n",
    "        num_patches (int): Total number of patches in the image.\n",
    "        projection_dim (int): Dimensionality of the projection space.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super(PatchEncoder, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = layers.Dense(units=projection_dim)\n",
    "        self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded\n",
    "\n",
    "# %%\n",
    "# Create the Vision Transformer (ViT) model\n",
    "def create_vit_classifier():\n",
    "    \"\"\"\n",
    "    Build a Vision Transformer (ViT) model for binary classification.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled ViT model.\n",
    "    \"\"\"\n",
    "    inputs = layers.Input(shape=(image_size, image_size, 1))\n",
    "\n",
    "    # Patch extraction\n",
    "    x = layers.Conv2D(filters=PROJECTION_DIM, kernel_size=PATCH_SIZE, strides=PATCH_SIZE, padding='valid')(inputs)\n",
    "    x = layers.Reshape((NUM_PATCHES, PROJECTION_DIM))(x)\n",
    "\n",
    "    # Positional Encoding\n",
    "    x = PatchEncoder(NUM_PATCHES, PROJECTION_DIM)(x)\n",
    "\n",
    "    # Transformer Encoder\n",
    "    for _ in range(TRANSFORMER_LAYERS):\n",
    "        x_res = x\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = layers.MultiHeadAttention(key_dim=PROJECTION_DIM, num_heads=NUM_HEADS, dropout=0.1)(x, x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.Add()([x, x_res])\n",
    "\n",
    "        x_res = x\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x = layers.Dense(TRANSFORMER_UNITS[0], activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "        x = layers.Dense(TRANSFORMER_UNITS[1])(x)\n",
    "        x = layers.Add()([x, x_res])\n",
    "\n",
    "    # MLP Head\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    for units in MLP_HEAD_UNITS:\n",
    "        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "    outputs = layers.Dense(NUM_CLASSES, activation='sigmoid')(x)\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d317941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_vit_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a64386b53589898",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T13:00:06.954829Z",
     "start_time": "2024-09-19T13:00:06.518005Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 8, 8, 64)     3136        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 64, 64)       0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " patch_encoder (PatchEncoder)   (None, 64, 64)       8256        ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 64, 64)      128         ['patch_encoder[0][0]']          \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " multi_head_attention (MultiHea  (None, 64, 64)      66368       ['layer_normalization[0][0]',    \n",
      " dAttention)                                                      'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 64, 64)       0           ['multi_head_attention[0][0]']   \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 64, 64)       0           ['dropout[0][0]',                \n",
      "                                                                  'patch_encoder[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 64, 64)      128         ['add[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64, 128)      8320        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64, 128)      0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 64, 64)       8256        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 64, 64)       0           ['dense_2[0][0]',                \n",
      "                                                                  'add[0][0]']                    \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 64, 64)      128         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (MultiH  (None, 64, 64)      66368       ['layer_normalization_2[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64, 64)       0           ['multi_head_attention_1[0][0]'] \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 64, 64)       0           ['dropout_2[0][0]',              \n",
      "                                                                  'add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 64, 64)      128         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64, 128)      8320        ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 64, 128)      0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64, 64)       8256        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 64, 64)       0           ['dense_4[0][0]',                \n",
      "                                                                  'add_2[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 64, 64)      128         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_2 (MultiH  (None, 64, 64)      66368       ['layer_normalization_4[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64, 64)       0           ['multi_head_attention_2[0][0]'] \n",
      "                                                                                                  \n",
      " add_4 (Add)                    (None, 64, 64)       0           ['dropout_4[0][0]',              \n",
      "                                                                  'add_3[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_5 (LayerNo  (None, 64, 64)      128         ['add_4[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64, 128)      8320        ['layer_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 64, 128)      0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64, 64)       8256        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " add_5 (Add)                    (None, 64, 64)       0           ['dense_6[0][0]',                \n",
      "                                                                  'add_4[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_6 (LayerNo  (None, 64, 64)      128         ['add_5[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " multi_head_attention_3 (MultiH  (None, 64, 64)      66368       ['layer_normalization_6[0][0]',  \n",
      " eadAttention)                                                    'layer_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 64, 64)       0           ['multi_head_attention_3[0][0]'] \n",
      "                                                                                                  \n",
      " add_6 (Add)                    (None, 64, 64)       0           ['dropout_6[0][0]',              \n",
      "                                                                  'add_5[0][0]']                  \n",
      "                                                                                                  \n",
      " layer_normalization_7 (LayerNo  (None, 64, 64)      128         ['add_6[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 64, 128)      8320        ['layer_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 64, 128)      0           ['dense_7[0][0]']                \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 64, 64)       8256        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " add_7 (Add)                    (None, 64, 64)       0           ['dense_8[0][0]',                \n",
      "                                                                  'add_6[0][0]']                  \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 64)          0           ['add_7[0][0]']                  \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 128)          8320        ['global_average_pooling1d[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 128)          0           ['dense_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 64)           8256        ['dropout_8[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 64)           0           ['dense_10[0][0]']               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 1)            65          ['dropout_9[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 360,833\n",
      "Trainable params: 360,833\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print model summary\n",
    "print(\"Model Summary:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b318998a359c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model: tf.keras.Model, train_ds: tf.data.Dataset, val_ds: tf.data.Dataset, \n",
    "                epochs: int, optimizer: tf.keras.optimizers.Optimizer, \n",
    "                loss: str, metrics: list[str], callbacks: list[tf.keras.callbacks.Callback]) -> tf.keras.callbacks.History:\n",
    "    \"\"\"\n",
    "    Compiles and trains a model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (tf.keras.Model): The model to train.\n",
    "    - train_ds (tf.data.Dataset): The training dataset.\n",
    "    - val_ds (tf.data.Dataset): The validation dataset.\n",
    "    - epochs (int): Number of epochs to train the model.\n",
    "    - optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for training.\n",
    "    - loss (str): The loss function to use for training.\n",
    "    - metrics (list[str]): A list of metrics to evaluate during training.\n",
    "    - callbacks (list[tf.keras.callbacks.Callback]): A list of callbacks to use during training.\n",
    "\n",
    "    Returns:\n",
    "    - tf.keras.callbacks.History: The history object containing the training details.\n",
    "    \"\"\"\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    return history\n",
    "\n",
    "def evaluate_and_save_results(model: tf.keras.Model, test_ds: tf.data.Dataset, metrics: list[str], results_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and saves the results to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - model (tf.keras.Model): The trained model to evaluate.\n",
    "    - test_ds (tf.data.Dataset): The test dataset.\n",
    "    - metrics (list[str]): A list of metrics to include in the evaluation.\n",
    "    - results_dir (str): Directory to save the evaluation results.\n",
    "    \"\"\"\n",
    "    test_results = model.evaluate(test_ds)\n",
    "\n",
    "    print(\"Test Loss:\", test_results[0])\n",
    "    for i, metric in enumerate(metrics):\n",
    "        print(f\"Test {metric}:\", test_results[i + 1])\n",
    "\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    results_path = os.path.join(results_dir, 'test_results_vit_markov.txt')\n",
    "    with open(results_path, 'w') as f:\n",
    "        f.write(f\"Test Loss: {test_results[0]}\\n\")\n",
    "        for i, metric in enumerate(metrics):\n",
    "            f.write(f\"Test {metric}: {test_results[i + 1]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667bd8871442e74a",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-19T13:00:07.103222Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 15:00:07.119718: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [112000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-09-19 15:00:07.119889: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [112000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-09-19 15:00:11.513069: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2024-09-19 15:00:11.571196: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-19 15:00:11.704579: I tensorflow/compiler/xla/service/service.cc:169] XLA service 0x7fa0b435c1d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-09-19 15:00:11.704613: I tensorflow/compiler/xla/service/service.cc:177]   StreamExecutor device (0): NVIDIA GeForce GTX 1070, Compute Capability 6.1\n",
      "2024-09-19 15:00:11.709791: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-09-19 15:00:11.772285: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-09-19 15:00:11.816466: I ./tensorflow/compiler/jit/device_compiler.h:180] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500/3500 [==============================] - ETA: 0s - loss: 0.1898 - accuracy: 0.9134"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-19 15:38:45.994017: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [24000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-09-19 15:38:45.995026: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [24000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500/3500 [==============================] - 2883s 821ms/step - loss: 0.1898 - accuracy: 0.9134 - val_loss: 0.1233 - val_accuracy: 0.9454 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "3500/3500 [==============================] - 951s 272ms/step - loss: 0.0820 - accuracy: 0.9676 - val_loss: 0.0578 - val_accuracy: 0.9778 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "3500/3500 [==============================] - 72s 21ms/step - loss: 0.0439 - accuracy: 0.9839 - val_loss: 0.0653 - val_accuracy: 0.9808 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "3500/3500 [==============================] - 72s 21ms/step - loss: 0.0344 - accuracy: 0.9880 - val_loss: 0.0327 - val_accuracy: 0.9894 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "3500/3500 [==============================] - 72s 21ms/step - loss: 0.0283 - accuracy: 0.9905 - val_loss: 0.0230 - val_accuracy: 0.9925 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "3500/3500 [==============================] - 72s 21ms/step - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.0238 - val_accuracy: 0.9923 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "3500/3500 [==============================] - 73s 21ms/step - loss: 0.0215 - accuracy: 0.9928 - val_loss: 0.0294 - val_accuracy: 0.9914 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "3500/3500 [==============================] - 73s 21ms/step - loss: 0.0195 - accuracy: 0.9936 - val_loss: 0.0222 - val_accuracy: 0.9935 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "3500/3500 [==============================] - 73s 21ms/step - loss: 0.0180 - accuracy: 0.9944 - val_loss: 0.0175 - val_accuracy: 0.9944 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "3500/3500 [==============================] - 73s 21ms/step - loss: 0.0173 - accuracy: 0.9943 - val_loss: 0.0584 - val_accuracy: 0.9843 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "3500/3500 [==============================] - 73s 21ms/step - loss: 0.0152 - accuracy: 0.9951 - val_loss: 0.0199 - val_accuracy: 0.9942 - lr: 1.0000e-04\n",
      "Epoch 12/30\n",
      "3500/3500 [==============================] - 1499s 428ms/step - loss: 0.0145 - accuracy: 0.9951 - val_loss: 0.0196 - val_accuracy: 0.9939 - lr: 1.0000e-04\n",
      "Epoch 13/30\n",
      "3276/3500 [===========================>..] - ETA: 3:03 - loss: 0.0135 - accuracy: 0.9955"
     ]
    }
   ],
   "source": [
    "# Addestramento del modello\n",
    "history = train_model(\n",
    "    model=model,\n",
    "    train_ds=train_ds_classification,\n",
    "    val_ds=val_ds_classification,\n",
    "    epochs=num_epochs,\n",
    "    optimizer= optimizer,\n",
    "    loss= loss,\n",
    "    metrics= metrics,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875f17622a2ded8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T12:58:21.503773318Z",
     "start_time": "2024-08-09T11:29:48.316537Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-09 13:29:48.325763: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [24000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-08-09 13:29:48.325938: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [24000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000/24000 [==============================] - 438s 18ms/step - loss: 0.0231 - accuracy: 0.9961\n",
      "Test Loss: 0.023110203444957733\n",
      "Test Accuracy: 0.9960833191871643\n"
     ]
    }
   ],
   "source": [
    "# Valutazione e visualizzazione dei risultati\n",
    "evaluate_and_save_results(\n",
    "    model=model,\n",
    "    test_ds=test_ds_classification,\n",
    "    metrics= metrics,\n",
    "    results_dir='results_vit'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b7f88f",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581bc105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to divide an image into patches\n",
    "def extract_patches(image, patch_size):\n",
    "    \"\"\"\n",
    "    Extracts square patches from an image using TensorFlow.\n",
    "\n",
    "    Parameters:\n",
    "        image (numpy.ndarray): Input image as a NumPy array with shape (H, W, C),\n",
    "                               where H is height, W is width, and C is the number of channels (e.g., 3 for RGB).\n",
    "        patch_size (int): The size of the square patches to extract (patch_size x patch_size).\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: An array of patches with shape (num_patches, patch_size, patch_size, C),\n",
    "                       where num_patches is the total number of patches extracted from the image.\n",
    "    \"\"\"\n",
    "    # Use TensorFlow's extract_patches function to divide the image into patches\n",
    "    patches = tf.image.extract_patches(\n",
    "        images=tf.expand_dims(image, 0),  # Add a batch dimension to the input image\n",
    "        sizes=[1, patch_size, patch_size, 1],  # Patch size configuration\n",
    "        strides=[1, patch_size, patch_size, 1],  # Sliding window strides\n",
    "        rates=[1, 1, 1, 1],  # Dilation rate (no dilation applied)\n",
    "        padding='VALID'  # Ensure patches fit completely within the image dimensions\n",
    "    )\n",
    "\n",
    "    # Reshape the extracted patches to remove the batch dimension and match the desired output shape\n",
    "    patches = tf.reshape(patches, (-1, patch_size, patch_size, image.shape[-1]))\n",
    "\n",
    "    return patches.numpy()  # Convert to NumPy array for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7542495b523a21dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-19T09:03:24.074291612Z",
     "start_time": "2024-08-09T11:37:06.436714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAIeCAYAAABqT3wbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAp+ElEQVR4nO3de3RV9Zn/8efkTsyNJBAgEdRKkEtCBFQQ5aYzagt0pCOjLunYGau11aroz6J1AG2rq8446gI7BbXU/qowziCdqXaQyjIoilYUtHKpIga5yS0XArlyzv794Y+UFOTszy6RZ2ber7VcXRyes5/ne85OPmcn9LtjQRAEBgAATqqUkz0AAAAgkAEAcIFABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkPG/wnPPPWcDBgyw3//+9yf0uDNmzLAJEyZEeu60adNs2rRpHX/etm2bDRgwwJ577rkTNd5/axMmTLAZM2ac7DGALwyBjC5zOAQP/1dRUWGXXHKJ3XfffbZ37175eD/96U/tpZde6oJJ/ZozZ85RgT9hwgS74YYbTtJEALpK2skeAP/zffe737WysjJra2uzt99+2xYuXGgrVqyw559/3rp16xb6OPPmzbNLLrnELr744i6cFl4sXbrUYrHYyR4D+MIQyOhyY8aMsYqKCjMzu+KKK6ygoMAWLFhgy5cvt4kTJ57k6eBVRkbGyR4B+ELxI2t84UaOHGlmn/3O1MzsySeftCuvvNLOO+88q6ystClTptjSpUs7PWfAgAHW1NRkS5Ys6fgR+JG/X9y1a5fdfffddsEFF9iQIUNswoQJNmvWLGtra+t0nLa2NnvggQds5MiRVlVVZd/5znestrY21NwvvfSSTZw40SoqKmzixIn229/+9ph1iUTCfv7zn9tXvvIVq6iosPPPP99mzpxpDQ0NoV8jxeHfPT/55JP29NNP20UXXWRDhw61v/u7v7OdO3daEAT22GOP2ZgxY6yystJuvPFGq6+v73SMwz8Gf/PNN23KlClWWVlpkyZNsjfffNPMzJYtW2aTJk2yiooKmzJliq1fv77T8zdu3GgzZsywiy66yCoqKmz06NF21113WV1d3VHzHu5RUVFhF198sS1atMjmzJljAwYMOGqmI9/jw78Cefvtt0O9hytWrLCrr77aqqqq7Oyzz7brr7/ePvzww6gvM9DluELGF+6TTz4xM7OCggIzM/vFL35hEyZMsEmTJll7e7u98MILdsstt9i8efNs3LhxZmb24IMP2j333GOVlZU2depUMzPr27evmX0Wxn/9139tjY2NNnXqVDvjjDNs165d9uKLL1pLS0unK60f/vCHlpeXZzfddJNt377dnnrqKbvvvvvskUceOe7MK1eutJtvvtnOPPNMu/32262urs7uuusu69Wr11G1M2fOtCVLltiUKVNs2rRptm3bNnv66adt/fr1tnDhQktPT/8zX8Fj+/Wvf23t7e02bdo0q6+vtyeeeMJuvfVWGzlypL355pv2zW9+07Zs2WK//OUv7cc//rE98MADnZ6/ZcsWu/322+3KK6+0yZMn289+9jP71re+Zffee689/PDDdtVVV5mZ2fz58+3WW2+1pUuXWkrKZ5/pX3/9ddu6datNmTLFevToYR9++KE9++yztmnTJnv22Wc7fvS8fv16u+6666xHjx528803WyKRsMcee8wKCwtDrzPMe/irX/3KZsyYYRdccIHdcccd1tzcbAsXLrSrr77alixZYmVlZX/mqw10gQDoIosXLw7Ky8uD119/Pdi3b1+wc+fO4IUXXgjOPffcoLKyMvj000+DIAiC5ubmTs9ra2sLJk6cGHz961/v9HhVVVXwve9976g+d955Z3DWWWcF77333lF/l0gkOs1y7bXXdjwWBEFw//33BwMHDgz2799/3LV89atfDUaPHt2pbuXKlUF5eXkwfvz4jsfeeuutoLy8PPjP//zPTs9/5ZVXjnr8mmuuCa655pqOP2/dujUoLy8PFi9efNxZxo8fH1x//fVHPW/kyJGd5nvooYeC8vLyYPLkyUF7e3vH49OnTw8GDx4ctLa2djpmeXl58M4773Q89uqrrwbl5eVBZWVlsH379o7HFy1aFJSXlwdvvPFGx2N/+h4GQRA8//zzQXl5efDWW291PHbDDTcEQ4cO7XjvgyAIampqgkGDBgXl5eVHrfPI9zvse3jgwIFgxIgRwT333NPpeHv27AmGDx9+1OOAF/zIGl3u2muvtVGjRtnYsWPttttus1NOOcXmzp1rJSUlZmaWlZXVUdvQ0GCNjY02fPjwo34seiyJRMJeeuklGz9+fMfvqY/0p/8oaOrUqZ0eGzFihMXjcdu+ffvn9ti9e7dt2LDBLr/8csvNze14fPTo0XbmmWd2ql26dKnl5uba6NGjrba2tuO/wYMHW3Z2dsePgLvCpZde2mm+yspKMzObPHmypaWldXq8vb3ddu3a1en5Z555pp199tkdfx46dKiZffYrhj59+hz1+NatWzseO/I9bG1ttdra2o66devWmZlZPB63VatW2UUXXdTx3puZ9evXzy688MLQ60z2Hr7++uu2f/9++8pXvtLpPUhJSbGhQ4d26XsA/Dn4kTW63MyZM+3000+31NRUKy4uttNPP73jR51mZi+//LL9y7/8i23YsKHT73zD/Avb2tpaO3DggPXv3z/ULEcGi5lZXl6emZnt37//c5+zY8cOM/ssOP7U6aef3umDw5YtW6yxsdFGjRp1zGPt27cv1JxR9O7du9OfD4fz5z3e0NBgp556atLn/+mP5XNycsys82tWX19vc+fOtd/85jdHrbGxsdHMPlt7S0vLMV/HYz32eZK9hzU1NWZm9rd/+7fHfP7h+QFvCGR0ucrKymNevZqZrV692m688UY755xzbNasWdajRw9LT0+3xYsX2/PPP3/CZznyg8CRgiA4IcdPJBJWVFRk//RP/3TMv1d+V6pKTU095uNh1/x5z/+8x498/q233mpr1qyxv//7v7eBAwdadna2JRIJu+66607Ya3tYsvUc/t8HH3zQevTocVTd560HONkIZJxUL774omVmZtqTTz7Z6R9fLV68ONTzCwsLLScnp0v/9ezhK7ItW7Yc9Xcff/xxpz/37dvXVq1aZcOGDev0Y9z/yRoaGmzVqlV2880320033dTx+OEr1cOKioosMzPzmK/jsR6L6vBVf1FRkZ1//vkn7LhAV+N3yDipUlNTLRaLWTwe73hs27Zttnz58qNqs7Ozj/rRckpKil188cX28ssvH3NbzBNxddazZ08bOHCgLVmypOPHr2Zmr732mm3atKlT7WWXXWbxeNx+8pOfHHWcQ4cOHfdH4/9dfd4V51NPPXVU3fnnn2/Lly/v9PvrLVu22KuvvnrC5rnwwgstJyfH5s2bZ+3t7Uf9fdj/mxvwReMKGSfV2LFjbcGCBXbdddfZxIkTbd++ffbMM89Y37597Q9/+EOn2sGDB9uqVatswYIF1rNnTysrK7OhQ4fa9OnT7bXXXrNp06bZ1KlT7Utf+pLt2bPHli5das8880zH7xj/HNOnT7cbbrjBrr76avva175m9fX19stf/tL69+9vTU1NHXXnnnuu/c3f/I3NmzfPNmzYYKNHj7b09HSrqamxpUuX2ve//3279NJL/+x5PMnJybFzzjnHnnjiCWtvb7eSkhJ77bXXOv5/5ke66aabbOXKlXbVVVfZVVddZYlEouN13LBhwwmbZ/bs2XbnnXfalClT7Mtf/rIVFhbajh07bMWKFTZs2DCbOXPmCekFnEgEMk6qUaNG2Y9+9CN7/PHH7f7777eysjK74447bPv27UcF8owZM2zmzJn2yCOPWEtLi11++eU2dOhQKykpsWeffdYeffRR+/Wvf20HDhywkpISGzNmzAn7sfGYMWPs0UcftUceecQeeugh69u3rz3wwAO2fPly+93vftep9r777rMhQ4bYokWL7OGHH7bU1FQrLS21yZMn27Bhw07IPN489NBD9oMf/MCeeeYZC4LARo8ebY8//vhR/3p6yJAh9vjjj9uDDz5ojz76qPXu3du++93v2ubNm23z5s0nbJ5JkyZZz549bf78+fbkk09aW1ublZSU2IgRI2zKlCknrA9wIsWCE/0vLgBA9O1vf9s2bdpky5YtO9mjACcNv0MG8IVqaWnp9Oeamhp75ZVX7Nxzzz1JEwE+8CNrAF+oiy++2C6//HI79dRTbfv27bZo0SJLT0+366677mSPBpxUBDKAL9SFF15oL7zwgu3Zs8cyMjKsqqrKpk+fbqeddtrJHg04qfgdMgAADvA7ZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHEgLWxiLVUsHLi1TRzH76U+1+gcf1Hu8smJc0pplv01ec6SqKn2Obt20+o8/1ntUVlSHqvv3xTHpuL/5jT7LR5u0+pwcvccLLwRJa8aM0daan6/PsW2bVh+L8LH4nbeTr3X8eG2tTU36HJ/uEut36j1aW5Ov1cwsI6Pr39uGBq0+PV3vcfBg8vVOnqStdeVr+hyZmVp9fb3eo7k53Hubmamt99zzIswinv+bN+s9amuPv16ukAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAgdBbZ5aK2wGeGmHrzOI9Wn2ZOFPo427X6kuL9B7Z6taZUdZaEa6sVFxvyW59lPo6rT6vVe8RRpE4R/dDeo+D4vaKKdqugKEVi2s90Kz3aD6g1ddFeD3Dimdo9c0RtmeNt2j1gThTWHt7aPUtp+g94uL3qHbxtVFkiudNUa3eo0n8nrOrC9bLFTIAAA4QyAAAOEAgAwDgAIEMAIADBDIAAA4QyAAAOEAgAwDgAIEMAIADBDIAAA4QyAAAOEAgAwDgAIEMAIADoW8u8dNvaQcu/oY6itlI8TlF/6D3COOZ+7T6BaV6jwxx0/lf7NV73HpZuLqFP9aOu2SXPsvaQKvvrrcIZf56rb5nhB7rUrX6rvpUPP/3Wn1DhEHWizcgWD5Y7xFW1bVa/baz9R7939PqDxTqPcJ4Y4FW33uu3qNFvGmO+toovimeN98Tv87NzBrEm7wsi/LNIQmukAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHYkEQhNpleMyr1dKBy8r0Ye59Wqv/WYS9rB8IxiWtabk3ec2Rqoboc3QT9wD+eIveo/7G6lB1u+7QNnF97lV9lvU7tPr8LL3HDz9MfirfVKattfAUfY6P9mn1UT4V/9+9ydd6S19trQda9TlqGrX6jRF6bI+H2wg9tURbb0qE71GJ7eITsvUe8c3J1ztgprbWj36lz5Gao9Uf2qb3iH8S7r3tk6qt9/Le+izq+b+mTu/x3qHjr5crZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAdC31wCAAB0Ha6QAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwIC1s4bJLYtKBy0rlWeyZmVr9PU/pPbJmJd+Y7K5YtXTMwVX6HN26afWffKz3uG3nuFB1N+8PV3fYyuf0WT59f4VUn1UwVu7x8T3VSWt63a6dx9lF+hy1m7S1xiJ8LK57Ivl53Od72lrbGvU5DtRo9YfW6T0ObQm3mWBpmrbevvootlWsz46wD+IH8eRPuv8Mba2Lt+hz5Irn5daE3uOjEGs1M0vrp623+Gv6LG0HtPqGt/Ue8bePv16ukAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAgdBbZ1at1Q5cuk+cxMwWbNfqq97Xe2wMUTN4rXbMoVX6HOrWmfk1eo+wBr6r1W+LsP2hfaiVZ+dG6BFCL3GO3D16jxRxm8KULvpY3OsDrb7loN4jZYdWX1ur9wirTtymMi07Qo8mrb4pVe8RxkZxa+JPd+k9GjK0+r3i1pMS8bwpEb/OzcxaxPe29VO9RzJcIQMA4ACBDACAAwQyAAAOEMgAADhAIAMA4ACBDACAAwQyAAAOEMgAADhAIAMA4ACBDACAAwQyAAAOEMgAADgQ+uYS3Zq1A2eL9WZmGW1avTpTVx03yhzqzSWyxI3PFer8Ga16j3TxORnixvZdNkeUtYrncayLPhars8fFuc3MUtu1+lhc7xFWIqbVt0e48YPaI95F722b+PVxKMIc6uujvjYK9bxRvwbN9PM/5ZDeI+kxT/whAQCAikAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAdC72X98enagT8uU0cx+0Wx2KOf3iOMT8S15p+m98gSe2wR683MLgjC1X1ymnbcXRHe29pGrb45T+8Rao7eWn1bd71HvbiHdFftZb1PXGvrAb3HQXH/38Q+vUdY2SHP98MKIuxB3yr2yBL3+g6rz16tPj/CnuyniPtHt3blPuXZWn1tL71Hm3j+t+7SeyTDFTIAAA4QyAAAOEAgAwDgAIEMAIADBDIAAA4QyAAAOEAgAwDgAIEMAIADBDIAAA4QyAAAOEAgAwDgAIEMAIADsSAIxO3SAQDAicYVMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADqSFLfz3K2LSgUtL5Vls4fe0+u8v1HuUTE++MdnNBdpaBw7V5+jWTav/pEbvMWtjuE3YvhEbJx33jcX6LLt/r/XILKiWe+y4JflzimbPlo6ZVazP0fCBVh+L8LG48ZHqpDXFP5wtHbN9f/Jj/qmmzSu0J7wnt7D2D8Kdx2dkal+3px3SZ9mSqtXnxPUe78aTr3duqbbWf/1UnyNXrN+ijWRmZusOhXtv08u1g+deNUuepb2xWqpveks8980s/urx18sVMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4EHrrzN98WTtwSU91FLMlJVr9wAv1HjeGqFk5RTvmtsH6HBmZWv2ubXqPsJvHvfE17bg1Yr2ZWdtarT7WXe8RRv1XxTkinMfBOvEJ4naMYdWJa7UGvUfaBq2+d5beI6wRp2r1lbv0Hut6aPUFzXqPMC4XZ9+l7oNpZt3btPp1hXqPsErP1eq3que+mXz+554SoUcSXCEDAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4EPrmEh+dqR24PsLNAdbGtPrZffQeYXw6RHxCf71HunhzidoIm8OHtbtinFTf9pHeI7XqXqk+0T3srTGOUBtijrO1OeI99Dmy1mv1QRd9LE4RX/PU/fpaczNWSPWlu+UWoQ0o0+orDuk9Dok9ihr1HmFUBVr93GK9R6F4Y4ygt94jrNKBWv0O8evczCy1QTv/i3dr534YXCEDAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADoTeyzrngHbgPHGvZjMzdfvr/Ba9RxhZ9Vp9doT9ajPatPrm/XqPsDLrq6X6WJ3eI1Gn7RMbdK/Wm4SZo1bcr7lWn+NQiD21jxRpL+ui5CWJ+q5fa5t4Lhysl1uEtv+gVl8X4ftHg9gjtUnvEYb6JRhlrSmtWn19F63VTD9v5K9zM7OGaqm8JcL3wWS4QgYAwAECGQAABwhkAAAcIJABAHCAQAYAwAECGQAABwhkAAAcIJABAHCAQAYAwAECGQAABwhkAAAcIJABAHAgFgRBcLKHAADgfzuukAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcCAtbOGYlTHpwEXd5Vls/iCt/t6deo+5fZJvTNbrYW2tvfrrc6RnavW1Edb60dfDbcJWdN9s6bj1/6HPkrrmXqk+UThL7nFo7+ykNRkx7b1N9NDnyFyv1Qepcgtr6j47aU1GurbW1AZ9rbnrtPe1/2/lFvba3eHO41njtfUOr9FnWdtXqy9q1Ht8553k6y1J1dY65wx9jqJmrf7dPnqP6b8L996Ovl9b7+q79VnU87/0Re3cNzP7cOrx18sVMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4EHrrzPwG7cDdQx/5j3qK9f9wUO8RRvY+rT63h94jQ9w6s61O7xFW1t5qqT62W+8R3yNuy1hYrTcJISHOEfTQ52hXX58IW2daiK1p1dc82F0tj9GyR6tvFL+2FPv2a/V7m7q+R9BF36PUUyzKWhMtWv2+CNuEhqWeN/Hd+jawQUO1VN+0V26RFFfIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADoS+BcS2Mu3AB/PVUczWxbX6HxXpPcKoPVOrT+mn90jP0OrrxZtRKBrKV0j1wWC9R9Ygrf5QoTaTmZltSF6SOahaOmT7YH2OnMFjpfogysfiEF8r6lrTxBvImJl1X69t4l+2Ve8RVvmpWv3AVr1Hs/h9sEi8GUVYg8UbkgyMcAOcIvGGFG2leo+wysq18+yjwdVyj3Tx/O/5on4Di2S4QgYAwAECGQAABwhkAAAcIJABAHCAQAYAwAECGQAABwhkAAAcIJABAHCAQAYAwAECGQAABwhkAAAcCL2XdSyhHTglUEfRPx101acJea1ifZQean2XHlvcc9zMLBB7qPWhj6vO/kWstYtOZPk1jLDWhPicRBeex3Hx2Gp9lOd01XrltzbC92P1OVFez7Dk1zHK1616LkfokQxXyAAAOEAgAwDgAIEMAIADBDIAAA4QyAAAOEAgAwDgAIEMAIADBDIAAA4QyAAAOEAgAwDgAIEMAIADBDIAAA7EgiCIsO04AAA4kbhCBgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAgbSwheNXxKQDF3eXZ7H5FVr97O16j0fLkm9M1ucftbX2KtfnyMjU6vft1Ht8+I1wm7AVz5wtHbfuP/RZUtbeK9UnCmbJPeINs5PWZMS19zbeQ58jc71WH0T4WNzcc3bSmvSYttbUBn2tOeu097X/b+UWtmpmuPP4+2O09Q7fos+y9lStvqhR73HLu8nXW5SqrXXOafocRc1a/bu99R53vh3uvR11n7bet+/WZ1HP/z4vaue+mdlHVx9/vVwhAwDgAIEMAIADBDIAAA4QyAAAOEAgAwDgAIEMAIADBDIAAA4QyAAAOEAgAwDgAIEMAIADobfObMrWDnygmzqKWUNC7CFuPxlWW65W33KK3iOeodW35ug9wmrPE7eAy9d7pOaN1Z5QqG9LZzY7+Rz52hxBT32ONLGHpcotwh1WnCM1v1rukbFe224wO09uEVqe+HWYH+H7R574fTDvkN4jjHzxUio/K0IP8fux+toosvO08yzKuZy2X3tOVr6+1WwyXCEDAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4EPrmEp+WaAdujnAzhPXNWn2NeBOIsA6cptWn9NF7pKZr9QfFm1Eoms7Q6tMG6j1y23pK9W3d9R6h5hiszdHSQ+/RfdAKqT4R5eYSm5KX5Ihrzaj/N3mMnoO0Hv3+S24R2oDTrpDqBzfp620WexTXyy1CGSTevGdQhO9RxY1afWtf7bVR9DtrvVT/e/G8NDPLGKSdD32W6j2S4QoZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcCD8Xta9tQPXhT7yHy1fp9VvHKz3COOQeNzaQr1HTNy/OFGk9witUivv3aK3KBW3fT1YoPcIo/9faPWNEV73slO1+kQXfSxW15q9X+/Rr59WP+4yvYcF4couG68ddmSePsrqKq2+eK/eoy5EzUWnace8bJQ+R1G9Vr9miN6jKWSdet58cLc8imWLe3ePinIubzz+X3OFDACAAwQyAAAOEMgAADhAIAMA4ACBDACAAwQyAAAOEMgAADhAIAMA4ACBDACAAwQyAAAOEMgAADhAIAMA4EAsCIKQW7cDAICuwhUyAAAOEMgAADhAIAMA4ACBDACAAwQyAAAOEMgAADhAIAMA4ACBDACAAwQyAAAOpIUtzDgUkw4cb5Nnsaqfa/Vrr9B7xHsk35isNKGttU4rNzOzhLg/WnaEHrWxcE3OaNcOPmKrPsuAbVr9/lP0Ho8OT77eWSu0te7L0+coF1+feISPxbdNTL7W76/U1pp3UJ9jQM1Yqf6yl/UemYuqQ9X9VWycdNwRb2j1ZmZrz6mW6ov2yC1sXknyHmtumi0dc/H1+hzd92r17w+rlnssKAj3nNYrx0nHfeBbK+RZcpu0+r94XTv3zcwqf1h93L/nChkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHAi9dWZ+g3bg5gPqKGbb1mj1KaP0HtYjeUlf8ZBp4pZrZmbtca2+oJvew9LDlZ0W+iz4TGWJPkqF2KMuS+8RxvDTtPq92XqPgZlafZStM8MY3k+rz2/VewzO1rYoHLlf7xH228KIkeOk41aMlEex1BFaj+4Rts60muQlZWdrh6w8Tx8jf59Wnz5snN5kY7iykcO18+wHp+uj5LVo9ecd0LfnbE7y91whAwDgAIEMAIADBDIAAA4QyAAAOEAgAwDgAIEMAIADBDIAAA4QyAAAOEAgAwDgAIEMAIADBDIAAA4QyAAAOBB6y/+GfO3A8Qg3B+hfqdXvLdV7hLFVrK+LcAOCRKDVt8b0HmFtEW90sS7ChvmHtmn1DafoPaxn8pK1n2iH3Jenj9EsrjXKzSUuOCt5zVrxRM6LcJOU5pqxUv3qtXqPsNa+VS3Vp64ep/cYofUojnJziRB2vKfVr1ut9+guPuf91dV6k5BWr9XOs/tH6jd+yBXP/9XrtJnC4AoZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcCD0Xtbp7dqBgzZ1FLMDteITIuy9G0a2uG90k7gXtJlZPKHVZ6XrPcLKSdXqC7rpPYpytfrUCPuDd8UcQYQ9tYvE/a8TXfSxWF1rXujvBn9UXCDWF+s96kLWFfXQjttdrDczKxafUxihh4XY5z6nsOvnKBDPy6II7621hCtTz5sfiee+mVmueP73EN+DMLhCBgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcCAWBEGIrcwBAEBX4goZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAH0sIWTn4hJh147x55FnvjG1p9/x/oPf5wT/KNye6v0da6cbs+R1ubVt+nWO/xzxXhNmGbu1Nb7+Ul+ixVWgur01vYoVjy9ZYE2iC7I8wxOK7VJyL0WJ+WfK1F4lrzIwwyqFmrv6hG73HbkHDn8ZqfzJaOW7ZGn2VHpVafU6v3+NKs2cmLYiFqjrDzJn2OrH1a/W7xtTEzGzBjdqi6h9/XzuUfD9JnyRf3rPzLCN8c5vQ+fhOukAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAgdBbZ64crR245aA6illvcXu3j/5K7xHG4n5a/acRtpI8JG5TmJ+p9/jnkHX/2ks77q5GeRSbu1err8vSe1if5CVzNmuH3JutjzFQ3DY2Lm4ramZmFclL5tRoh8xv0ccYtEOrv2yV3uO2IeHqFs/Xjlv5pj7LuhFafWGELYRvmZW8Zudc7ZjLI2ydmS9unbnxbL3H/5kRrm7er7Tjzonw/UM9/y9Zq/eYc83x/54rZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAdC31wis1U7cLxZHcWsRdzMPPWA3iOMXPHGDw1teo/2uFZ/SqreI+y7mysetnuGPIkVdtPqUyLcTCOMInGORIRN6ovEG1JEurlEmDnEteZH+HheLJ48RQV6j7C6F2v1+UUReog3lygQv5eElSXOHmWtueJzCobpPcJSzxv1a9DMLF/8HluUr/dIhitkAAAcIJABAHCAQAYAwAECGQAABwhkAAAcIJABAHCAQAYAwAECGQAABwhkAAAcIJABAHCAQAYAwIHQe1nXF2gHbo+wB3D/Sq2+vkzvEcZW8WPK3hy9RyLQ6luj7GUd0hZxlnW1eo9gp1ZfH2Ev2isKk9e8u0M75j51o28za9uu1ccjfCw++0vJa94VX/O8Jn2O1k/GSvVr3td7hPX+O9VSffo74/QeI7QeRXvlFqHsfk+r37hG71Egzr5uWLXeJKQ172vn2bvnrZB75In3X1izUZvJzMwmHf+vuUIGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwIBYEgXhrAQAAcKJxhQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgAMEMgAADhDIAAA4QCADAOAAgQwAgANpYQsz22PSgTNDH/mPvrlOq184WO+xI5Z8Y7K0rdparVCfI5aq1Sea9B7xwnCbsKVv0tZb+jt9ltINV0j1Bwv+Te6x9vbk6x19z1TpmI1F+hxlH2j1iQgfi5c+lnyto2Zra83er6+130btfR33X3IL+3rwbKi61me19Y58W1/v6iptvcV75RZWd3Py9T5cqX3NzvsrfY6ieq1+zRDttTEza7o+3Hv7i5j23s6/Wx7Fshu182HUMn299248/nq5QgYAwAECGQAABwhkAAAcIJABAHCAQAYAwAECGQAABwhkAAAcIJABAHCAQAYAwAECGQAAB2JBEITaX/HCldpWbUURtpOcf5ZWf++neo/H+iRfbq9HtLWW9NfnSM/Q6msjrHXztHBbZxb+YLZ03Ib/0GdJXXOvVJ8onCX3OLRndtKazBTtvY331OfIFLeANXEbVTOzgwWzk9ZkZGhrTW3Q15q7Xntfy5fJLWzlXeHO49kTtPWO+FifZU1frb6oUe/x7XdCfI9K09Y65wx9jiJxu953S/Uet70Z7r294AFtvW9F2Dozdb92/pct1c59M7MPrjj+erlCBgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcIBABgDAAQIZAAAHCGQAABwgkAEAcCAtbGFztnbgpkx1FLOGcPuMdzgQoUcYbTlafYv42piZxcWbS6gzKdpzxU3S8/UeqfljxR76xu1ms0/4HEGEOdLVHhFuLhGGuta0vGq5R8Z6bUP+7Fy5RWi54tdhXlbX98g9pPcII1+714LlR1hrflyrz+um9wgrO1c7z1IjnMvq+Z/1on4zlmS4QgYAwAECGQAABwhkAAAcIJABAHCAQAYAwAECGQAABwhkAAAcIJABAHCAQAYAwAECGQAABwhkAAAcCL2X9eYztAPvirB36rLdWv2aEr1HGA3DtfrWXnqPlNCv/P/v0UVrNTNrOkerzz2o9yju2VOqb+mu9wij9BJtjqZivUfPS1ZI9Ykoe1kvS17SR1xrVsO/yWP0uVTrMeq/5Bah/cX5V0j15+Xo6109WOvRo1ZuEcpfai+7XVKl9yhq0OrXnKW9NopRf7leqq95VXyBTD//q8SvLzMzS3K/Bq6QAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHIgFQZBku2sAANDVuEIGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMABAhkAAAcIZAAAHCCQAQBwgEAGAMCB/wdidZUVkKTvOAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 600x600 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load a sample image path from the training DataFrame\n",
    "sample_image_path = train_df.iloc[0][\"image_path\"]  # Replace train_df with your actual DataFrame\n",
    "\n",
    "# Load the image from the file (assuming it is stored as a NumPy .npy file)\n",
    "sample_image = np.load(sample_image_path).astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# Define the patch size\n",
    "PATCH_SIZE = 32  # Example patch size\n",
    "\n",
    "# Extract patches from the image\n",
    "patches = extract_patches(sample_image, PATCH_SIZE)\n",
    "\n",
    "# Visualize the original image\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(sample_image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Visualize the extracted patches\n",
    "NUM_PATCHES = patches.shape[0]  # Total number of patches\n",
    "plt.figure(figsize=(6, 6))\n",
    "for i, patch in enumerate(patches):\n",
    "    plt.subplot(int(np.sqrt(NUM_PATCHES)), int(np.sqrt(NUM_PATCHES)), i + 1)\n",
    "    plt.imshow(patch)\n",
    "    plt.axis(\"off\")\n",
    "plt.suptitle(\"Image Patches\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f895fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate and plot the confusion matrix\n",
    "def plot_confusion_matrix(test_ds, model, class_labels):\n",
    "    \"\"\"\n",
    "    Calculate and plot the confusion matrix with percentages for a classification model.\n",
    "\n",
    "    Parameters:\n",
    "    - test_ds (Dataset): A dataset containing test image batches and their corresponding labels.\n",
    "    - model (Model): The trained classification model.\n",
    "    - class_labels (list): A list of strings representing class labels for the confusion matrix.\n",
    "\n",
    "    Returns:\n",
    "    - cm (ndarray): The confusion matrix as a 2D numpy array.\n",
    "    - cm_percent (ndarray): The confusion matrix with percentages as a 2D numpy array.\n",
    "    \"\"\"\n",
    "    # Collect ground truth labels (y_true) and model predictions (y_pred)\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for image_batch, label_batch in test_ds:\n",
    "        predictions = model.predict(image_batch)\n",
    "        y_true.extend(label_batch.numpy())  # Append ground truth labels\n",
    "        y_pred.extend([round(pred.item()) for pred in predictions])  # Append rounded predictions\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Calculate percentage for each cell relative to the total samples for that class\n",
    "    cm_percent = cm / cm.sum(axis=1, keepdims=True) * 100\n",
    "\n",
    "    # Plot the confusion matrix with both absolute numbers and percentages\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='', cmap='Blues',\n",
    "                xticklabels=class_labels, yticklabels=class_labels)\n",
    "\n",
    "    # Add percentages to the heatmap\n",
    "    for i in range(len(cm)):\n",
    "        for j in range(len(cm)):\n",
    "            plt.text(j + 0.25, i + 0.5, f'{cm_percent[i][j]:.2f}%', \n",
    "                     ha='center', va='center', color='black')\n",
    "\n",
    "    # Set axis labels and title\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix (with Percentages)')\n",
    "    plt.show()\n",
    "\n",
    "    return cm, cm_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae73f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = [\"Class 0\", \"Class 1\"]\n",
    "cm, cm_percent = plot_confusion_matrix(test_ds=test_ds_classification, model=model, class_labels=class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29604fad",
   "metadata": {},
   "source": [
    "# 6. Model Saving\n",
    "\n",
    "After training, we save the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d772eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model: tf.keras.Model, model_save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    - model (tf.keras.Model): The trained model to save.\n",
    "    - model_save_path (str): Path where the model will be saved.\n",
    "    \"\"\"\n",
    "    # Creazione della directory se non esiste\n",
    "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
    "\n",
    "    # Save\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "save_model(model, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
