{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66ef858c",
   "metadata": {},
   "source": [
    "# 1. Imports and Setup\n",
    "\n",
    "In this cell, we import all the necessary libraries and set up the working environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T08:08:35.288498Z",
     "start_time": "2024-07-28T08:07:51.271392Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-28 10:07:57.338679: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-28 10:08:12.921694: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow and Keras modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, Model, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# scikit-learn for data splitting\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56f8f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load configuration from a JSON file\n",
    "def load_config(filename: str = \"config-Unet.json\") -> dict:\n",
    "    \"\"\"\n",
    "    Loads the configuration from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Configuration parameters loaded from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c79734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image paths\n",
    "data_dir = \"/media/neurone-pc6/Volume/Michele/Prog_GAF_Michele/pythonProject/data/GADF\"\n",
    "label_0_folder = os.path.join(data_dir, \"Label_0\")\n",
    "label_1_folder = os.path.join(data_dir, \"Label_1\")\n",
    "\n",
    "# Directory paths for saving metadata, results, and models\n",
    "metadata_dir = 'metadata'\n",
    "results_dir = 'results'\n",
    "model_dir = 'models'\n",
    "\n",
    "# Paths for logger and model saving\n",
    "logger_path = os.path.join(results_dir, 'training_u-net_32x32_gadf.log')\n",
    "model_save_path = os.path.join(model_dir, 'u-net_gadf.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45a5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = config[\"unet\"][\"training\"][\"batch_size\"]\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "INPUT_SHAPE = (32, 32, 1)\n",
    "EPOCHS = config[\"unet\"][\"training\"][\"epochs\"]\n",
    "\n",
    "\n",
    "# Define steps per epoch and validation steps based on dataset cardinality\n",
    "STEPS_PER_EPOCH = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "VALIDATION_STEPS = tf.data.experimental.cardinality(val_ds).numpy()\n",
    "\n",
    "# Define optimizer, loss, and metrics from configuration\n",
    "OPTIMIZER = tf.keras.optimizers.get({\n",
    "    \"class_name\": config[\"unet\"][\"training\"][\"optimizer\"],\n",
    "    \"config\": {\n",
    "        \"learning_rate\": config[\"unet\"][\"training\"][\"learning_rate\"]\n",
    "    }\n",
    "})\n",
    "\n",
    "LOSS = config[\"unet\"][\"training\"][\"loss\"]\n",
    "METRICS = config[\"unet\"][\"training\"][\"metrics\"]\n",
    "\n",
    "# Configure callbacks for training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=config[\"unet\"][\"training\"][\"early_stopping\"][\"monitor\"],\n",
    "    patience=config[\"unet\"][\"training\"][\"early_stopping\"][\"patience\"],\n",
    "    restore_best_weights=config[\"unet\"][\"training\"][\"early_stopping\"][\"restore_best_weights\"]\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=config[\"unet\"][\"training\"][\"model_checkpoint\"][\"filepath\"],\n",
    "    monitor=config[\"unet\"][\"training\"][\"model_checkpoint\"][\"monitor\"],\n",
    "    save_best_only=config[\"unet\"][\"training\"][\"model_checkpoint\"][\"save_best_only\"]\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=config[\"unet\"][\"training\"][\"lr_scheduler\"][\"monitor\"],\n",
    "    factor=config[\"unet\"][\"training\"][\"lr_scheduler\"][\"factor\"],\n",
    "    patience=config[\"unet\"][\"training\"][\"lr_scheduler\"][\"patience\"]\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(logger_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd72005",
   "metadata": {},
   "source": [
    "# 2. Utility Functions\n",
    "\n",
    "In this section, we define the utility functions for loading configuration, data, and creating the U-Net model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca036c7df79236f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T08:08:35.337458Z",
     "start_time": "2024-07-28T08:08:35.294499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to create a pandas DataFrame with image paths and labels\n",
    "def create_dataframe(label_0_folder: str, label_1_folder: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Creates a DataFrame containing image paths and labels.\n",
    "\n",
    "    Parameters:\n",
    "    - label_0_folder (str): Directory containing images for label 0.\n",
    "    - label_1_folder (str): Directory containing images for label 1.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with columns 'image_path' and 'label'.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for filename in os.listdir(label_0_folder):\n",
    "        img_path = os.path.join(label_0_folder, filename)\n",
    "        data.append((img_path, 0))\n",
    "\n",
    "    for filename in os.listdir(label_1_folder):\n",
    "        img_path = os.path.join(label_1_folder, filename)\n",
    "        data.append((img_path, 1))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[\"image_path\", \"label\"])\n",
    "    return df\n",
    "\n",
    "# Function to load and preprocess an image\n",
    "def load_and_preprocess_image(image_path: tf.Tensor, label: int) -> tuple:\n",
    "    \"\"\"\n",
    "    Loads and preprocesses an image from a file path, normalizing it and adding a channel dimension if needed.\n",
    "\n",
    "    Parameters:\n",
    "    - image_path (tf.Tensor): Tensor containing the path to the image.\n",
    "    - label (int): Label associated with the image.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: The preprocessed image and its label.\n",
    "    \"\"\"\n",
    "    def _load_image(image_path):\n",
    "        image = np.load(image_path.decode('utf-8')).astype(np.float32)\n",
    "        # Normalize image values from [-1, 1] to [0, 1]\n",
    "        image = (image + 1) / 2.0\n",
    "        if image.ndim == 2:  # If the image is grayscale\n",
    "            image = np.expand_dims(image, axis=-1)  # Add channel dimension\n",
    "        return image\n",
    "\n",
    "    image = tf.numpy_function(_load_image, [image_path], tf.float32)\n",
    "    image.set_shape([32, 32, 1])  # Explicitly set shape for TensorFlow compatibility\n",
    "    return image, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0635be",
   "metadata": {},
   "source": [
    "# 3. U-Net Model Creation\n",
    "\n",
    "In this section, we define the U-Net model with its encoder and decoder blocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a20da02edc08a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T16:35:04.391685Z",
     "start_time": "2024-07-24T16:35:04.298769Z"
    }
   },
   "outputs": [],
   "source": [
    "def unet(input_shape: tuple[int, int, int]) -> Model:\n",
    "    \"\"\"\n",
    "    Builds a U-Net model for image segmentation.\n",
    "\n",
    "    Parameters:\n",
    "    - input_shape (tuple[int, int, int]): Shape of the input image (height, width, channels).\n",
    "\n",
    "    Returns:\n",
    "    - Model: A compiled U-Net model.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "\n",
    "    # Decoder\n",
    "    up4 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    up4 = Conv2D(64, (2, 2), activation='relu', padding='same')(up4)\n",
    "    up4 = Concatenate()([up4, conv2])\n",
    "\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(up4)\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv4)\n",
    "\n",
    "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    up5 = Conv2D(32, (2, 2), activation='relu', padding='same')(up5)\n",
    "    up5 = Concatenate()([up5, conv1])\n",
    "\n",
    "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(up5)\n",
    "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(conv5)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330500fe",
   "metadata": {},
   "source": [
    "# 4. Dataset Creation\n",
    "\n",
    "In this section, we create the training, validation, and test datasets from the previously created dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b933ae23d4b99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-28T08:08:35.991899Z",
     "start_time": "2024-07-28T08:08:35.935624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creation of the pandas DataFrame with image paths and labels\n",
    "df = create_dataframe(label_0_folder, label_1_folder)\n",
    "df[\"label\"] = df[\"label\"].astype(np.float32)  # Convert labels to float32 for compatibility with TensorFlow\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Function to create a TensorFlow dataset from a DataFrame\n",
    "def create_tf_dataset(df: pd.DataFrame, batch_size: int, autotune: int) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Converts a DataFrame containing image paths and labels into a TensorFlow dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame with columns 'image_path' and 'label'.\n",
    "    - batch_size (int): Number of samples per batch.\n",
    "    - autotune (int): Number of parallel calls to optimize performance.\n",
    "\n",
    "    Returns:\n",
    "    - tf.data.Dataset: A batched and prefetched TensorFlow dataset ready for training.\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df[\"image_path\"].values, df[\"label\"].values))\n",
    "    ds = ds.map(load_and_preprocess_image, num_parallel_calls=autotune)\n",
    "    ds = ds.batch(batch_size).prefetch(autotune)\n",
    "    return ds\n",
    "\n",
    "# Create TensorFlow datasets for training, validation, and testing\n",
    "train_ds = create_tf_dataset(train_df, BATCH_SIZE, AUTOTUNE)\n",
    "val_ds = create_tf_dataset(val_df, BATCH_SIZE, AUTOTUNE)\n",
    "test_ds = create_tf_dataset(test_df, 1, AUTOTUNE)  # Batch size of 1 for test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dcfe49",
   "metadata": {},
   "source": [
    "# 5. Model Training and Evaluation\n",
    "\n",
    "We will now train the U-Net model and evaluate it on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45889fd3403d0868",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T18:21:01.316266Z",
     "start_time": "2024-07-24T16:35:05.355816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 18:35:05.374863: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [128000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-07-24 18:35:05.375038: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [128000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-07-24 18:35:05.976153: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:424] Loaded cuDNN version 8600\n",
      "2024-07-24 18:35:06.054256: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - ETA: 0s - loss: 0.0042 - mse: 0.0042 - mae: 0.0388"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 19:17:36.344561: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [16000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-07-24 19:17:36.344969: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [16000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 2919s 1s/step - loss: 0.0042 - mse: 0.0042 - mae: 0.0388 - val_loss: 8.1907e-04 - val_mse: 8.1907e-04 - val_mae: 0.0203 - lr: 1.0000e-04\n",
      "Epoch 2/30\n",
      "2000/2000 [==============================] - 1444s 722ms/step - loss: 5.9379e-04 - mse: 5.9379e-04 - mae: 0.0172 - val_loss: 4.3291e-04 - val_mse: 4.3291e-04 - val_mae: 0.0147 - lr: 1.0000e-04\n",
      "Epoch 3/30\n",
      "2000/2000 [==============================] - 101s 51ms/step - loss: 3.1727e-04 - mse: 3.1727e-04 - mae: 0.0124 - val_loss: 2.2452e-04 - val_mse: 2.2452e-04 - val_mae: 0.0102 - lr: 1.0000e-04\n",
      "Epoch 4/30\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 1.8472e-04 - mse: 1.8472e-04 - mae: 0.0095 - val_loss: 1.3388e-04 - val_mse: 1.3388e-04 - val_mae: 0.0079 - lr: 1.0000e-04\n",
      "Epoch 5/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.1997e-04 - mse: 1.1997e-04 - mae: 0.0077 - val_loss: 1.1282e-04 - val_mse: 1.1282e-04 - val_mae: 0.0077 - lr: 1.0000e-04\n",
      "Epoch 6/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 8.4159e-05 - mse: 8.4159e-05 - mae: 0.0064 - val_loss: 6.5609e-05 - val_mse: 6.5609e-05 - val_mae: 0.0055 - lr: 1.0000e-04\n",
      "Epoch 7/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 6.3842e-05 - mse: 6.3842e-05 - mae: 0.0056 - val_loss: 5.0625e-05 - val_mse: 5.0625e-05 - val_mae: 0.0048 - lr: 1.0000e-04\n",
      "Epoch 8/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 5.0631e-05 - mse: 5.0631e-05 - mae: 0.0050 - val_loss: 4.0887e-05 - val_mse: 4.0887e-05 - val_mae: 0.0044 - lr: 1.0000e-04\n",
      "Epoch 9/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 4.1515e-05 - mse: 4.1515e-05 - mae: 0.0045 - val_loss: 3.6523e-05 - val_mse: 3.6523e-05 - val_mae: 0.0043 - lr: 1.0000e-04\n",
      "Epoch 10/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 3.4644e-05 - mse: 3.4644e-05 - mae: 0.0042 - val_loss: 2.9742e-05 - val_mse: 2.9742e-05 - val_mae: 0.0039 - lr: 1.0000e-04\n",
      "Epoch 11/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.6202e-05 - mse: 2.6202e-05 - mae: 0.0035 - val_loss: 2.4127e-05 - val_mse: 2.4127e-05 - val_mae: 0.0033 - lr: 5.0000e-05\n",
      "Epoch 12/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.3964e-05 - mse: 2.3964e-05 - mae: 0.0034 - val_loss: 2.1464e-05 - val_mse: 2.1464e-05 - val_mae: 0.0032 - lr: 5.0000e-05\n",
      "Epoch 13/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 2.1501e-05 - mse: 2.1501e-05 - mae: 0.0032 - val_loss: 1.9208e-05 - val_mse: 1.9208e-05 - val_mae: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 14/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.9357e-05 - mse: 1.9357e-05 - mae: 0.0031 - val_loss: 1.8403e-05 - val_mse: 1.8403e-05 - val_mae: 0.0030 - lr: 5.0000e-05\n",
      "Epoch 15/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.7737e-05 - mse: 1.7737e-05 - mae: 0.0030 - val_loss: 1.5875e-05 - val_mse: 1.5875e-05 - val_mae: 0.0027 - lr: 5.0000e-05\n",
      "Epoch 16/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.5499e-05 - mse: 1.5499e-05 - mae: 0.0027 - val_loss: 1.6333e-05 - val_mse: 1.6333e-05 - val_mae: 0.0029 - lr: 2.5000e-05\n",
      "Epoch 17/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.4785e-05 - mse: 1.4785e-05 - mae: 0.0027 - val_loss: 1.4131e-05 - val_mse: 1.4131e-05 - val_mae: 0.0026 - lr: 2.5000e-05\n",
      "Epoch 18/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.3998e-05 - mse: 1.3998e-05 - mae: 0.0026 - val_loss: 1.3363e-05 - val_mse: 1.3363e-05 - val_mae: 0.0025 - lr: 2.5000e-05\n",
      "Epoch 19/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.3325e-05 - mse: 1.3325e-05 - mae: 0.0025 - val_loss: 1.2742e-05 - val_mse: 1.2742e-05 - val_mae: 0.0025 - lr: 2.5000e-05\n",
      "Epoch 20/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.2739e-05 - mse: 1.2739e-05 - mae: 0.0025 - val_loss: 1.2188e-05 - val_mse: 1.2188e-05 - val_mae: 0.0024 - lr: 2.5000e-05\n",
      "Epoch 21/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.2228e-05 - mse: 1.2228e-05 - mae: 0.0024 - val_loss: 1.1740e-05 - val_mse: 1.1740e-05 - val_mae: 0.0024 - lr: 2.5000e-05\n",
      "Epoch 22/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.1775e-05 - mse: 1.1775e-05 - mae: 0.0024 - val_loss: 1.1281e-05 - val_mse: 1.1281e-05 - val_mae: 0.0023 - lr: 2.5000e-05\n",
      "Epoch 23/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.1368e-05 - mse: 1.1368e-05 - mae: 0.0023 - val_loss: 1.0872e-05 - val_mse: 1.0872e-05 - val_mae: 0.0023 - lr: 2.5000e-05\n",
      "Epoch 24/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.0998e-05 - mse: 1.0998e-05 - mae: 0.0023 - val_loss: 1.0511e-05 - val_mse: 1.0511e-05 - val_mae: 0.0022 - lr: 2.5000e-05\n",
      "Epoch 25/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.0415e-05 - mse: 1.0415e-05 - mae: 0.0022 - val_loss: 1.0288e-05 - val_mse: 1.0288e-05 - val_mae: 0.0022 - lr: 1.2500e-05\n",
      "Epoch 26/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 1.0221e-05 - mse: 1.0221e-05 - mae: 0.0022 - val_loss: 1.0056e-05 - val_mse: 1.0056e-05 - val_mae: 0.0022 - lr: 1.2500e-05\n",
      "Epoch 27/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 9.9786e-06 - mse: 9.9786e-06 - mae: 0.0022 - val_loss: 9.8097e-06 - val_mse: 9.8097e-06 - val_mae: 0.0022 - lr: 1.2500e-05\n",
      "Epoch 28/30\n",
      "2000/2000 [==============================] - 70s 35ms/step - loss: 9.7481e-06 - mse: 9.7481e-06 - mae: 0.0022 - val_loss: 9.5696e-06 - val_mse: 9.5696e-06 - val_mae: 0.0021 - lr: 1.2500e-05\n",
      "Epoch 29/30\n",
      "2000/2000 [==============================] - 71s 35ms/step - loss: 9.5289e-06 - mse: 9.5289e-06 - mae: 0.0021 - val_loss: 9.3715e-06 - val_mse: 9.3715e-06 - val_mae: 0.0021 - lr: 1.2500e-05\n",
      "Epoch 30/30\n",
      "2000/2000 [==============================] - 72s 36ms/step - loss: 9.2801e-06 - mse: 9.2801e-06 - mae: 0.0021 - val_loss: 9.2453e-06 - val_mse: 9.2453e-06 - val_mae: 0.0021 - lr: 6.2500e-06\n"
     ]
    }
   ],
   "source": [
    "def train_unet_model(model: Model, train_ds: tf.data.Dataset, val_ds: tf.data.Dataset, \n",
    "                     epochs: int, optimizer: tf.keras.optimizers.Optimizer, \n",
    "                     loss: str, metrics: list[str], callbacks: list[tf.keras.callbacks.Callback]) -> tf.keras.callbacks.History:\n",
    "    \"\"\"\n",
    "    Compiles and trains a U-Net model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (Model): The U-Net model to train.\n",
    "    - train_ds (tf.data.Dataset): The training dataset.\n",
    "    - val_ds (tf.data.Dataset): The validation dataset.\n",
    "    - epochs (int): Number of epochs to train the model.\n",
    "    - optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for training.\n",
    "    - loss (str): The loss function to use for training.\n",
    "    - metrics (list[str]): A list of metrics to evaluate during training.\n",
    "    - callbacks (list[tf.keras.callbacks.Callback]): A list of callbacks to use during training.\n",
    "\n",
    "    Returns:\n",
    "    - tf.keras.callbacks.History: The history object containing the training details.\n",
    "    \"\"\"\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "# Model generation and compilation\n",
    "model = unet(INPUT_SHAPE)\n",
    "\n",
    "# Train the U-Net model using the defined function\n",
    "history = train_unet_model(\n",
    "    model=model,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=OPTIMIZER,\n",
    "    loss=LOSS,\n",
    "    metrics=METRICS,\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr, csv_logger]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b40e78",
   "metadata": {},
   "source": [
    "# 7. Test Evaluation and Saving Results\n",
    "\n",
    "Evaluate the model on the test set and save the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7e5c5b44513fc8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T18:25:22.144245Z",
     "start_time": "2024-07-24T18:21:01.357713Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 20:21:01.362260: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [16000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2024-07-24 20:21:01.362473: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype float and shape [16000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 261s 16ms/step - loss: 9.2619e-06 - mse: 9.2619e-06 - mae: 0.0021\n",
      "Test Loss: 9.261943887395319e-06\n",
      "Test Accuracy: 9.261943887395319e-06\n"
     ]
    }
   ],
   "source": [
    "def evaluate_and_save_results(model: Model, test_ds: tf.data.Dataset, metrics: list[str], results_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and saves the results to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - model (Model): The trained model to evaluate.\n",
    "    - test_ds (tf.data.Dataset): The test dataset.\n",
    "    - metrics (list[str]): A list of metrics to include in the evaluation.\n",
    "    - results_dir (str): Directory to save the evaluation results.\n",
    "    \"\"\"\n",
    "    # Evaluate the model on the test dataset\n",
    "    test_results = model.evaluate(test_ds)\n",
    "\n",
    "    # Print the results to the console\n",
    "    print(\"Test Loss:\", test_results[0])\n",
    "    print(\"Test Accuracy:\", test_results[1])\n",
    "\n",
    "    # Save the results to a text file\n",
    "    test_results_path = os.path.join(results_dir, 'test_results_u-net_32x32_gadf.txt')\n",
    "    with open(test_results_path, 'w') as f:\n",
    "        f.write(f\"Test Loss: {test_results[0]}\\n\")\n",
    "        for i, metric in enumerate(metrics):\n",
    "            f.write(f\"Test {metric}: {test_results[i + 1]}\\n\")\n",
    "\n",
    "# Evaluate the model and save the results\n",
    "evaluate_and_save_results(model, test_ds, METRICS, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a676d0",
   "metadata": {},
   "source": [
    "# 6. Model Saving\n",
    "\n",
    "After training, we save the trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c3e818b83f6d8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T18:25:22.301909Z",
     "start_time": "2024-07-24T18:25:22.178077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to models/u-net_32x32.h5\n"
     ]
    }
   ],
   "source": [
    "def save_model(model: Model, model_save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    - model (Model): The trained model to save.\n",
    "    - model_save_path (str): Path where the model will be saved.\n",
    "    \"\"\"\n",
    "    # Save the model to the specified path\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the trained model\n",
    "save_model(model, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
