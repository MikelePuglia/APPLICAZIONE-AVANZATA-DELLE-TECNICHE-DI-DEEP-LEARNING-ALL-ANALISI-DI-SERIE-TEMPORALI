{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports and Setup\n",
    "\n",
    "In this cell, we import all the necessary libraries and set up the working environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# TensorFlow and Keras modules\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input, Model, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "# scikit-learn for data splitting\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to load configuration from a JSON file\n",
    "def load_config(filename: str = \"config-Unet.json\") -> dict:\n",
    "    \"\"\"\n",
    "    Loads the configuration from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - filename (str): The path to the configuration file.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Configuration parameters loaded from the JSON file.\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load configuration\n",
    "config = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Parameter to select dataset\n",
    "DATASET_TYPE = \"GASF\"  # Change to \"MARKOV\" or \"RGB\" to switch dataset\n",
    "\n",
    "# Paths based on dataset type\n",
    "if DATASET_TYPE == \"GASF\":\n",
    "    data_dir = \"/media/neurone-pc6/Volume/Michele/Prog_GAF_Michele/pythonProject/data/GASF\"\n",
    "    logger_path = os.path.join(\"results\", \"training_u-net_32x32_gasf.log\")\n",
    "    model_save_path = os.path.join(\"models\", \"u-net_gasf.h5\")\n",
    "    test_results_path = os.path.join(\"results\", \"test_results_u-net_32x32_gasf.txt\")\n",
    "elif DATASET_TYPE == \"GADF\":\n",
    "    data_dir = \"/media/neurone-pc6/Volume/Michele/Prog_GAF_Michele/pythonProject/data/GADF\"\n",
    "    logger_path = os.path.join(\"results\", \"training_u-net_32x32_gadf.log\")\n",
    "    model_save_path = os.path.join(\"models\", \"u-net_gadf.h5\")\n",
    "    test_results_path = os.path.join(\"results\", \"test_results_u-net_32x32_gadf.txt\")\n",
    "elif DATASET_TYPE == \"MARKOV\":\n",
    "    data_dir = \"/media/neurone-pc6/Volume/Michele/Prog_GAF_Michele/pythonProject/data/MARKOV\"\n",
    "    logger_path = os.path.join(\"results\", \"training_u-net_32x32_markov.log\")\n",
    "    model_save_path = os.path.join(\"models\", \"u-net_markov.h5\")\n",
    "    test_results_path = os.path.join(\"results\", \"test_results_u-net_32x32_markov.txt\")\n",
    "elif DATASET_TYPE == \"RGB\":\n",
    "    data_dir = \"/media/neurone-pc6/Volume/Michele/Prog_GAF_Michele/pythonProject/data/RGB\"\n",
    "    logger_path = os.path.join(\"results\", \"training_u-net_32x32_rgb.log\")\n",
    "    model_save_path = os.path.join(\"models\", \"u-net_rgb.h5\")\n",
    "    test_results_path = os.path.join(\"results\", \"test_results_u-net_32x32_rgb.txt\")\n",
    "\n",
    "# Directories for labels\n",
    "label_0_folder = os.path.join(data_dir, \"Label_0\")\n",
    "label_1_folder = os.path.join(data_dir, \"Label_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = config[\"unet\"][\"training\"][\"batch_size\"]\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "INPUT_SHAPE = (32, 32, 1)\n",
    "EPOCHS = config[\"unet\"][\"training\"][\"epochs\"]\n",
    "\n",
    "\n",
    "# Define steps per epoch and validation steps based on dataset cardinality\n",
    "STEPS_PER_EPOCH = tf.data.experimental.cardinality(train_ds).numpy()\n",
    "VALIDATION_STEPS = tf.data.experimental.cardinality(val_ds).numpy()\n",
    "\n",
    "# Define optimizer, loss, and metrics from configuration\n",
    "OPTIMIZER = tf.keras.optimizers.get({\n",
    "    \"class_name\": config[\"unet\"][\"training\"][\"optimizer\"],\n",
    "    \"config\": {\n",
    "        \"learning_rate\": config[\"unet\"][\"training\"][\"learning_rate\"]\n",
    "    }\n",
    "})\n",
    "\n",
    "LOSS = config[\"unet\"][\"training\"][\"loss\"]\n",
    "METRICS = config[\"unet\"][\"training\"][\"metrics\"]\n",
    "\n",
    "# Configure callbacks for training\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor=config[\"unet\"][\"training\"][\"early_stopping\"][\"monitor\"],\n",
    "    patience=config[\"unet\"][\"training\"][\"early_stopping\"][\"patience\"],\n",
    "    restore_best_weights=config[\"unet\"][\"training\"][\"early_stopping\"][\"restore_best_weights\"]\n",
    ")\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    filepath=config[\"unet\"][\"training\"][\"model_checkpoint\"][\"filepath\"],\n",
    "    monitor=config[\"unet\"][\"training\"][\"model_checkpoint\"][\"monitor\"],\n",
    "    save_best_only=config[\"unet\"][\"training\"][\"model_checkpoint\"][\"save_best_only\"]\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor=config[\"unet\"][\"training\"][\"lr_scheduler\"][\"monitor\"],\n",
    "    factor=config[\"unet\"][\"training\"][\"lr_scheduler\"][\"factor\"],\n",
    "    patience=config[\"unet\"][\"training\"][\"lr_scheduler\"][\"patience\"]\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(logger_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_image(image_path, dataset_type):\n",
    "    image = np.load(image_path).astype(np.float32)\n",
    "    if dataset_type == \"GASF\":\n",
    "        # Normalize values from [-1, 1] to [0, 1]\n",
    "        image = (image + 1) / 2.0\n",
    "        if image.ndim == 2:  # Grayscale images\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "    elif dataset_type == \"GADF\":\n",
    "        # Preprocessing specific for GADF: normalize to [0, 1]\n",
    "        image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "        if image.ndim == 2:  # Ensure channel dimension for grayscale images\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "    elif dataset_type == \"MARKOV\":\n",
    "        # Custom preprocessing for MARKOV (if needed)\n",
    "        image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "        if image.ndim == 2:\n",
    "            image = np.expand_dims(image, axis=-1)\n",
    "    elif dataset_type == \"RGB\":\n",
    "         image = np.load(image_path.decode('utf-8')).astype(np.float32)\n",
    "         image = np.expand_dims(image, axis=-1)  # Add channel dimension\n",
    "    return image\n",
    "\n",
    "    image = tf.numpy_function(_load_image, [image_path], tf.float32)\n",
    "    image.set_shape([32, 32, 3])  # Explicitly set shape for TensorFlow compatibility\n",
    "    # Normalizzazione dei pixel da [0, 255] a [0, 1]\n",
    "    image = image / 255.0\n",
    "    return image, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example usage of preprocess function\n",
    "# image_path = \"example_path.npy\"\n",
    "# processed_image = preprocess_image(image_path, DATASET_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def unet(input_shape: tuple[int, int, int]) -> Model:\n",
    "    \"\"\"\n",
    "    Builds a U-Net model for image segmentation.\n",
    "\n",
    "    Parameters:\n",
    "    - input_shape (tuple[int, int, int]): Shape of the input image (height, width, channels).\n",
    "\n",
    "    Returns:\n",
    "    - Model: A compiled U-Net model.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "\n",
    "    # Decoder\n",
    "    up4 = UpSampling2D(size=(2, 2))(conv3)\n",
    "    up4 = Conv2D(64, (2, 2), activation='relu', padding='same')(up4)\n",
    "    up4 = Concatenate()([up4, conv2])\n",
    "\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(up4)\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv4)\n",
    "\n",
    "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
    "    up5 = Conv2D(32, (2, 2), activation='relu', padding='same')(up5)\n",
    "    up5 = Concatenate()([up5, conv1])\n",
    "\n",
    "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(up5)\n",
    "    conv5 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv5)\n",
    "\n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(conv5)\n",
    "\n",
    "    # Create model\n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creation of the pandas DataFrame with image paths and labels\n",
    "df = create_dataframe(label_0_folder, label_1_folder)\n",
    "df[\"label\"] = df[\"label\"].astype(np.float32)  # Convert labels to float32 for compatibility with TensorFlow\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "# Function to create a TensorFlow dataset from a DataFrame\n",
    "def create_tf_dataset(df: pd.DataFrame, batch_size: int, autotune: int) -> tf.data.Dataset:\n",
    "    \"\"\"\n",
    "    Converts a DataFrame containing image paths and labels into a TensorFlow dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame with columns 'image_path' and 'label'.\n",
    "    - batch_size (int): Number of samples per batch.\n",
    "    - autotune (int): Number of parallel calls to optimize performance.\n",
    "\n",
    "    Returns:\n",
    "    - tf.data.Dataset: A batched and prefetched TensorFlow dataset ready for training.\n",
    "    \"\"\"\n",
    "    ds = tf.data.Dataset.from_tensor_slices((df[\"image_path\"].values, df[\"label\"].values))\n",
    "    ds = ds.map(load_and_preprocess_image, num_parallel_calls=autotune)\n",
    "    ds = ds.batch(batch_size).prefetch(autotune)\n",
    "    return ds\n",
    "\n",
    "# Create TensorFlow datasets for training, validation, and testing\n",
    "train_ds = create_tf_dataset(train_df, BATCH_SIZE, AUTOTUNE)\n",
    "val_ds = create_tf_dataset(val_df, BATCH_SIZE, AUTOTUNE)\n",
    "test_ds = create_tf_dataset(test_df, 1, AUTOTUNE)  # Batch size of 1 for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_unet_model(model: Model, train_ds: tf.data.Dataset, val_ds: tf.data.Dataset, \n",
    "                     epochs: int, optimizer: tf.keras.optimizers.Optimizer, \n",
    "                     loss: str, metrics: list[str], callbacks: list[tf.keras.callbacks.Callback]) -> tf.keras.callbacks.History:\n",
    "    \"\"\"\n",
    "    Compiles and trains a U-Net model.\n",
    "\n",
    "    Parameters:\n",
    "    - model (Model): The U-Net model to train.\n",
    "    - train_ds (tf.data.Dataset): The training dataset.\n",
    "    - val_ds (tf.data.Dataset): The validation dataset.\n",
    "    - epochs (int): Number of epochs to train the model.\n",
    "    - optimizer (tf.keras.optimizers.Optimizer): The optimizer to use for training.\n",
    "    - loss (str): The loss function to use for training.\n",
    "    - metrics (list[str]): A list of metrics to evaluate during training.\n",
    "    - callbacks (list[tf.keras.callbacks.Callback]): A list of callbacks to use during training.\n",
    "\n",
    "    Returns:\n",
    "    - tf.keras.callbacks.History: The history object containing the training details.\n",
    "    \"\"\"\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_ds,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "# Model generation and compilation\n",
    "model = unet(INPUT_SHAPE)\n",
    "\n",
    "# Train the U-Net model using the defined function\n",
    "history = train_unet_model(\n",
    "    model=model,\n",
    "    train_ds=train_ds,\n",
    "    val_ds=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    optimizer=OPTIMIZER,\n",
    "    loss=LOSS,\n",
    "    metrics=METRICS,\n",
    "    callbacks=[early_stopping, model_checkpoint, reduce_lr, csv_logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_and_save_results(model: Model, test_ds: tf.data.Dataset, metrics: list[str], results_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Evaluates the model on the test dataset and saves the results to a file.\n",
    "\n",
    "    Parameters:\n",
    "    - model (Model): The trained model to evaluate.\n",
    "    - test_ds (tf.data.Dataset): The test dataset.\n",
    "    - metrics (list[str]): A list of metrics to include in the evaluation.\n",
    "    - results_dir (str): Directory to save the evaluation results.\n",
    "    \"\"\"\n",
    "    # Evaluate the model on the test dataset\n",
    "    test_results = model.evaluate(test_ds)\n",
    "\n",
    "    # Print the results to the console\n",
    "    print(\"Test Loss:\", test_results[0])\n",
    "    print(\"Test Accuracy:\", test_results[1])\n",
    "\n",
    "    # Save the results to a text file\n",
    "    test_results_path = os.path.join(results_dir, 'test_results_u-net_32x32_markov.txt')\n",
    "    with open(test_results_path, 'w') as f:\n",
    "        f.write(f\"Test Loss: {test_results[0]}\\n\")\n",
    "        for i, metric in enumerate(metrics):\n",
    "            f.write(f\"Test {metric}: {test_results[i + 1]}\\n\")\n",
    "\n",
    "# Evaluate the model and save the results\n",
    "evaluate_and_save_results(model, test_ds, METRICS, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model: Model, model_save_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Saves the trained model to the specified path.\n",
    "\n",
    "    Parameters:\n",
    "    - model (Model): The trained model to save.\n",
    "    - model_save_path (str): Path where the model will be saved.\n",
    "    \"\"\"\n",
    "    # Save the model to the specified path\n",
    "    model.save(model_save_path)\n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the trained model\n",
    "save_model(model, model_save_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
